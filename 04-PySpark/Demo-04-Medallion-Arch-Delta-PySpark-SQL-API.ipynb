{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209e78d4-6dc7-4719-b542-88ba738bef17",
   "metadata": {},
   "source": [
    "## Demo 4: PySpark Medallion Architecture - Persisted in Delta Tables & Queried with the SQL API\n",
    "### Overview\n",
    "Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive. You can use the Dataset/DataFrame API in Scala, Java, Python or R to express streaming aggregations, event-time windows, stream-to-batch joins, etc. The computation is executed on the same optimized Spark SQL engine. Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs. In short, Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.\n",
    "\n",
    "Internally, by default, Structured Streaming queries are processed using a micro-batch processing engine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees. However, since Spark 2.3, we have introduced a new low-latency processing mode called Continuous Processing, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees. Without changing the Dataset/DataFrame operations in your queries, you will be able to choose the mode based on your application requirements.\n",
    "\n",
    "Having gained a better understanding of how to perform **incremental data processing** in a previous demonstration, where we combined Structured Streaming APIs and Spark SQL, we can now explore the tight integration between Structured Streaming and Delta Lake.\n",
    "\n",
    "#### Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "* Describe Bronze, Silver, and Gold tables\n",
    "* Create a Delta Lake multi-hop pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5e407-e677-4459-ae7f-700708621297",
   "metadata": {},
   "source": [
    "#### Incremental Updates in the Lakehouse\n",
    "Delta Lake allows users to easily combine streaming and batch workloads in a unified multi-stage pipeline, wherein each stage of the pipeline represents a state of our data valuable to driving core use cases within the business. With all data and metadata resident in object storage in the cloud, multiple users and applications can access data in near-real time, allowing analysts to access the freshest data as it's being processed.\n",
    "\n",
    "![](https://files.training.databricks.com/images/sslh/multi-hop-simple.png)\n",
    "\n",
    "- **Bronze** tables contain raw data ingested from various sources (JSON files, RDBMS data,  IoT data, to name a few examples).\n",
    "- **Silver** tables provide a more refined view of our data. We can join fields from various bronze tables to enrich streaming records, or update account statuses based on recent activity.\n",
    "- **Gold** tables provide business level aggregates often used for reporting and dashboarding. This would include aggregations such as daily active website users, weekly sales per store, or gross revenue per quarter by department. \n",
    "\n",
    "The end outputs are actionable insights, dashboards and reports of business metrics.  By considering our business logic at all steps of the ETL pipeline, we can ensure that storage and compute costs are optimized by reducing unnecessary duplication of data and limiting ad hoc querying against full historic data.  Each stage can be configured as a batch or streaming job, and ACID transactions ensure that we succeed or fail completely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53ffe38-53d5-4ae2-85a8-06c910f267d6",
   "metadata": {},
   "source": [
    "### Lab Details:\n",
    "This lab will demonstrate ingesting artificially generated medical data, in JSON format, that simulates heart rate monitor signals captured from numerous devices; therefore, this data represents what would be expected from a Streaming data source.\n",
    "\n",
    "#### Datasets Used:\n",
    "The schema of our two datasets is represented below. Note that we will be manipulating these schema during various steps.\n",
    "\n",
    "##### Recordings:\n",
    "The main dataset uses heart rate recordings from medical devices delivered in the JSON format.\n",
    "\n",
    "| Field | Type |\n",
    "| --- | --- |\n",
    "| device_id | int |\n",
    "| mrn | long |\n",
    "| time | double |\n",
    "| heartrate | double |\n",
    "\n",
    "##### Personally Identifiable Information (PII):\n",
    "These data will later be joined with a static table of patient information stored in an external system to identify patients.\n",
    "\n",
    "| Field | Type |\n",
    "| --- | --- |\n",
    "| mrn | long |\n",
    "| name | string |\n",
    "\n",
    "### 1.0. Prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af67835d-0dd5-461c-9f66-ad60bc27ff74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark-3.5.4-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b270a6-c516-4e23-8b99-2575f3880165",
   "metadata": {},
   "source": [
    "#### 1.1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2254708f-fdba-42af-ae1f-add9b66b07a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6732d0-0f54-4d36-b205-1cc4d2ad831b",
   "metadata": {},
   "source": [
    "#### 1.2. Instantiate Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1450fbb2-eb89-470e-89e9-c69218446ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = os.path.join(os.getcwd(), 'lab_data')\n",
    "data_dir = os.path.join(base_dir, 'healthcare')\n",
    "batch_dir = os.path.join(data_dir, 'batch')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "tracker_stream_dir = os.path.join(stream_dir, 'tracker')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Directory Structure for Data Lakehouse Files\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"healthcare_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database)\n",
    "\n",
    "patient_output_bronze = os.path.join(database_dir, 'dim_patient')\n",
    "heartbeat_output_bronze = os.path.join(database_dir, 'fact_heartbeat_bronze_checkpoint')\n",
    "heartbeat_output_silver = os.path.join(database_dir, 'fact_heartbeat_silver_checkpoint')\n",
    "heartbeat_output_gold = os.path.join(database_dir, 'fact_heartbeat_gold_checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797cf1ff-cda7-4544-bf9d-2035d48c3a58",
   "metadata": {},
   "source": [
    "#### 1.3. Define Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "772508ae-5a5c-400d-bff6-50068dc3ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_directory_tree(path: str):\n",
    "    '''If it exists, remove the entire contents of a directory structure at a given 'path' parameter's location.'''\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' has been removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "\n",
    "def wait_until_stream_is_ready(query, min_batches=1):\n",
    "    while len(query.recentProgress) < min_batches:\n",
    "        time.sleep(5)\n",
    "        \n",
    "    print(f\"The stream has processed {len(query.recentProgress)} batchs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4695da09-b727-4763-b75d-a5ec4c9da951",
   "metadata": {},
   "source": [
    "#### 1.4. Create a New Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ccd2d6-e814-4038-ae64-2805ea61be43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://JT-5570:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[10]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Delta Data Lakehouse Heartrate Monitor</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1d70f277ec0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "shuffle_partitions = int(os.cpu_count())\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder \\\n",
    "    .appName('PySpark Delta Data Lakehouse Heartrate Monitor')\\\n",
    "    .master(worker_threads)\\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .config('spark.executor.memory', '2g')\\\n",
    "    .config('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .config('spark.sql.debug.maxToStringFields', 50) \\\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog') \\\n",
    "    .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension') \\\n",
    "    .config('spark.sql.shuffle.partitions', shuffle_partitions) \\\n",
    "    .config('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .config('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .config('spark.sql.warehouse.dir', database_dir) \\\n",
    "    .config('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e707f388-2c30-419c-b14c-f5cc68be6891",
   "metadata": {},
   "source": [
    "#### 1.5. Initialize Data Lakehouse Directory Structure\n",
    "Remove the Data Lakehouse Database Directory Structure to Ensure Idempotency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86748010-6c2f-4f06-8e45-a1e0fe03876f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Directory 'C:\\\\Users\\\\jtupi\\\\Documents\\\\UVA\\\\DS-2002-Teacher\\\\04-PySpark\\\\spark-warehouse\\\\healthcare_dlh' has been removed successfully.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_directory_tree(database_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a3c5f4-e64e-4263-9b6c-3debdc586486",
   "metadata": {},
   "source": [
    "#### 1.6. Create a New Metadata Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dccef9ea-522d-4a35-859a-00e505300cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {dest_database} CASCADE;\")\n",
    "\n",
    "sql_create_db = f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {dest_database}\n",
    "    COMMENT 'DS-2002 Demo 04 Database'\n",
    "    WITH DBPROPERTIES (contains_pii = true, purpose = 'DS-2002 Demo 4');\n",
    "\"\"\"\n",
    "spark.sql(sql_create_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b55ad1-194e-4796-bdcc-dc1b00a99336",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 2.0. Create Bronze Layer\n",
    "#### 2.1. Read a Batch of Patient dimension data from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2c1ef5f-f581-42c7-8520-1dd980df764b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtupi\\Documents\\UVA\\DS-2002-Teacher\\04-PySpark\\lab_data\\healthcare\\batch\\patient_info.csv\n"
     ]
    }
   ],
   "source": [
    "patient_csv = os.path.join(batch_dir, 'patient_info.csv')\n",
    "print(patient_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4d740c9-d6de-4229-86a0-9b44390884fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mrn: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "The 'df_patients' table contains 34 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mrn</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23940128</td>\n",
       "      <td>Caitlin Garcia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18064290</td>\n",
       "      <td>Anthony Perez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95384990</td>\n",
       "      <td>Tanya Diaz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53057176</td>\n",
       "      <td>Autumn Calderon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96005424</td>\n",
       "      <td>Ronald Smith</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mrn             name\n",
       "0  23940128   Caitlin Garcia\n",
       "1  18064290    Anthony Perez\n",
       "2  95384990       Tanya Diaz\n",
       "3  53057176  Autumn Calderon\n",
       "4  96005424     Ronald Smith"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_patient = spark.read.format('csv').options(header='true', inferSchema=True).load(patient_csv)\n",
    "\n",
    "# Unit Test ---------------------------------------------------------\n",
    "df_patient.printSchema()\n",
    "print(f\"The 'df_patients' table contains {df_patient.count()} rows.\")\n",
    "df_patient.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1e8d3c-823e-4339-ac75-871f688ab1dc",
   "metadata": {},
   "source": [
    "#### 2.2. Persist the Patient dimension data to a Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e391257d-e7eb-44d5-a879-af0c97bbc8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patient.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{dest_database}.dim_patients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744f7d9d-f3ca-4213-8da6-d9df611239ac",
   "metadata": {},
   "source": [
    "#### 2.3. Read Patient Data from the Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db8e4871-dda4-4f70-b186-fc04838ac11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mrn</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23940128</td>\n",
       "      <td>Caitlin Garcia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18064290</td>\n",
       "      <td>Anthony Perez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95384990</td>\n",
       "      <td>Tanya Diaz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53057176</td>\n",
       "      <td>Autumn Calderon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96005424</td>\n",
       "      <td>Ronald Smith</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mrn             name\n",
       "0  23940128   Caitlin Garcia\n",
       "1  18064290    Anthony Perez\n",
       "2  95384990       Tanya Diaz\n",
       "3  53057176  Autumn Calderon\n",
       "4  96005424     Ronald Smith"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_patient = spark.read.format(\"delta\").table(f\"{dest_database}.dim_patients\")\n",
    "df_patient.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e49c8e-4c34-4c5d-89d9-6f0ed8deec02",
   "metadata": {},
   "source": [
    "#### 2.4. Use Structured Streaming to Read Heartrate Monitor data\n",
    "##### Read data from a series of JSON source files into a streaming DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b75f7db-aabe-404e-a558-6d0921e8cfa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tracker = (spark.readStream \\\n",
    "              .option(\"schemaLocation\", heartbeat_output_bronze) \\\n",
    "              .option(\"maxFilesPerTrigger\", 1) \\\n",
    "              .option(\"multiLine\", \"true\") \\\n",
    "              .json(tracker_stream_dir)\n",
    "             )\n",
    "\n",
    "df_tracker.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a626f964-9fc1-4eb1-bbde-fa1adc644968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- heartrate: string (nullable = true)\n",
      " |-- mrn: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unit Test -------------\n",
    "print(type(df_tracker))\n",
    "df_tracker.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dabadb8-9f56-416a-8d83-404a14a2a30f",
   "metadata": {},
   "source": [
    "#### 2.5. Write Data from Streaming DataFrame into a 'Bronze' Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "469ab7ae-400d-4684-bb59-0ec6e3d84589",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_query = (df_tracker.writeStream \\\n",
    "                .format(\"delta\") \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .queryName(\"heartbeat_tracker_bronze\")\n",
    "                .trigger(availableNow = True) \\\n",
    "                .option(\"checkpointLocation\", heartbeat_output_bronze) \\\n",
    "                .option(\"compression\", \"snappy\") \\\n",
    "                .toTable(f\"{dest_database}.fact_heartbeat_bronze\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28498f0c-bfc0-49de-aa00-0a0fb676a143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 98037071-110a-4395-8d07-1ec644d46b46\n",
      "Query Name: heartbeat_tracker_bronze\n",
      "Query Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n",
      "Last Progress: None\n"
     ]
    }
   ],
   "source": [
    "# Unit Test ----------------------------------\n",
    "print(f\"Query ID: {bronze_query.id}\")\n",
    "print(f\"Query Name: {bronze_query.name}\")\n",
    "print(f\"Query Status: {bronze_query.status}\")\n",
    "print(f\"Last Progress: {bronze_query.lastProgress}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79049ebc-2d89-4d8e-8739-5ed9d6ac9bbf",
   "metadata": {},
   "source": [
    "#### 2.6. Verify Bronze Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f39b2f02-47f9-4e11-b4af-4e7d7517d54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>healthcare_dlh</td>\n",
       "      <td>dim_patients</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>healthcare_dlh</td>\n",
       "      <td>fact_heartbeat_bronze</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        namespace              tableName  isTemporary\n",
       "0  healthcare_dlh           dim_patients        False\n",
       "1  healthcare_dlh  fact_heartbeat_bronze        False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"USE {dest_database};\")\n",
    "spark.sql(\"SHOW TABLES\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ff79c4a-d9df-4863-85b2-aff008b07d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bbe9b7-c5b3-4450-803f-291fcb6449b6",
   "metadata": {},
   "source": [
    "### 3.0. Create Silver Layer\n",
    "#### 3.1. Define Silver Query to Join Streaming with Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57ef3f36-cea0-48bc-8c32-3f6cc0d3eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .table(f\"{dest_database}.fact_heartbeat_bronze\") \\\n",
    "    .join(df_patient, \"mrn\") \\\n",
    "    .select(col(\"device_id\").cast(IntegerType()), \\\n",
    "            col(\"mrn\").cast(LongType()), \\\n",
    "            (col(\"time\")/1e6).cast(TimestampType()).alias(\"datetime\"), \\\n",
    "            col(\"heartrate\").cast(DoubleType()), \\\n",
    "            col(\"name\").alias(\"patient_name\")\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a80c45b-71ae-4f41-9493-4eb4ae40c615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- mrn: long (nullable = true)\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- heartrate: double (nullable = true)\n",
      " |-- patient_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unit Test -----------\n",
    "df_silver.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf5bff-3d78-43c4-aa62-cfe56dbc2357",
   "metadata": {},
   "source": [
    "#### 3.2. Persist Data to a 'Silver' Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8d425ce-63ac-446c-8dc6-1ce7d133069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_query = (df_silver.writeStream \\\n",
    "                .format(\"delta\") \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .queryName(\"heartbeat_tracker_silver\")\n",
    "                .trigger(availableNow = True) \\\n",
    "                .option(\"checkpointLocation\", heartbeat_output_silver) \\\n",
    "                .option(\"compression\", \"snappy\") \\\n",
    "                .toTable(f\"{dest_database}.fact_heartbeat_silver\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27422a0e-185e-4bdd-993e-9dbfc5ef0816",
   "metadata": {},
   "source": [
    "#### 3.3. Verify Silver Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b452d3cc-8c93-4dad-9007-35ff60fec78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>healthcare_dlh</td>\n",
       "      <td>dim_patients</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>healthcare_dlh</td>\n",
       "      <td>fact_heartbeat_bronze</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>healthcare_dlh</td>\n",
       "      <td>fact_heartbeat_silver</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        namespace              tableName  isTemporary\n",
       "0  healthcare_dlh           dim_patients        False\n",
       "1  healthcare_dlh  fact_heartbeat_bronze        False\n",
       "2  healthcare_dlh  fact_heartbeat_silver        False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"USE {dest_database};\")\n",
    "spark.sql(\"SHOW TABLES\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66b27cbe-6dd6-438f-9f18-41b593206cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab1f48-0275-488d-8dfa-de4f9065decf",
   "metadata": {},
   "source": [
    "### 4.0. Create Gold Layer\n",
    "#### 4.1. Define Gold Query to Perform an Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4edc0481-c30f-47a1-b621-1cdd6d8360c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heartrate_by_patient_gold = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .table(f\"{dest_database}.fact_heartbeat_silver\") \\\n",
    "    .groupBy('patient_name') \\\n",
    "    .agg((ceiling(avg(\"heartrate\")).alias(\"avg_heartrate\")), \\\n",
    "        (count(\"device_id\").alias(\"count\"))) \\\n",
    "    .orderBy(desc(\"avg_heartrate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c96d533-bbb6-4640-ae82-aaa4e41ecdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- patient_name: string (nullable = true)\n",
      " |-- avg_heartrate: long (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unit Test -----\n",
    "df_heartrate_by_patient_gold.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a6689-7277-46e3-ac86-1f2518643d44",
   "metadata": {},
   "source": [
    "#### 4.2. Persist Gold Data to a 'Gold' Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "730bccb7-1a7f-4f94-8a3e-d331753d3a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_query = (df_heartrate_by_patient_gold.writeStream \\\n",
    "              .outputMode(\"complete\") \\\n",
    "              .queryName(\"fact_heartbeat_by_patient_gold\") \\\n",
    "              .format(\"delta\") \\\n",
    "              .trigger(availableNow = True) \\\n",
    "              .option(\"checkpointLocation\", heartbeat_output_gold) \\\n",
    "              .option(\"compression\", \"snappy\") \\\n",
    "              .toTable(f\"{dest_database}.fact_heartbeat_by_patient_gold\")\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35262c91-2e04-40d6-8246-a4f4ba8102ec",
   "metadata": {},
   "source": [
    "#### 4.3. Read Gold Streaming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7f23127-029f-4698-9199-f151e71cb124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- patient_name: string (nullable = true)\n",
      " |-- avg_heartrate: long (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact_heartbeat_by_patient_gold = spark.sql(f\"SELECT * FROM {dest_database}.fact_heartbeat_by_patient_gold\")\n",
    "df_fact_heartbeat_by_patient_gold.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba9e89b-7470-4e50-bcb5-65313109e870",
   "metadata": {},
   "source": [
    "#### 4.4. Display the Gold table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c902220-2af2-4eb7-b6e2-7a913732b389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stream has processed 1 batchs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_name</th>\n",
       "      <th>avg_heartrate</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anthony Perez</td>\n",
       "      <td>75</td>\n",
       "      <td>2850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dr. Amanda Baxter</td>\n",
       "      <td>75</td>\n",
       "      <td>4640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Melissa Martinez</td>\n",
       "      <td>75</td>\n",
       "      <td>2487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crystal Ho</td>\n",
       "      <td>75</td>\n",
       "      <td>2973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Autumn Calderon</td>\n",
       "      <td>75</td>\n",
       "      <td>1819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sharon Brewer</td>\n",
       "      <td>86</td>\n",
       "      <td>1819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rachel Contreras</td>\n",
       "      <td>86</td>\n",
       "      <td>4423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Valerie Reese</td>\n",
       "      <td>86</td>\n",
       "      <td>4326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Joshua Perkins</td>\n",
       "      <td>83</td>\n",
       "      <td>3484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Valerie Garcia</td>\n",
       "      <td>83</td>\n",
       "      <td>2726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Samuel Hughes</td>\n",
       "      <td>83</td>\n",
       "      <td>4138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Caitlin Garcia</td>\n",
       "      <td>80</td>\n",
       "      <td>3322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Gabriela Gibson</td>\n",
       "      <td>80</td>\n",
       "      <td>3280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sarah Kennedy</td>\n",
       "      <td>80</td>\n",
       "      <td>4661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ashley Schmidt</td>\n",
       "      <td>79</td>\n",
       "      <td>3234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Lynn Russell</td>\n",
       "      <td>79</td>\n",
       "      <td>3716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Ronald Smith</td>\n",
       "      <td>79</td>\n",
       "      <td>2942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Michelle Carter</td>\n",
       "      <td>85</td>\n",
       "      <td>3514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Nicholas Spears</td>\n",
       "      <td>85</td>\n",
       "      <td>3134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>John Estes</td>\n",
       "      <td>85</td>\n",
       "      <td>3264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>William Gomez Jr.</td>\n",
       "      <td>104</td>\n",
       "      <td>1069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Robert Vincent</td>\n",
       "      <td>100</td>\n",
       "      <td>3171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Cynthia Figueroa</td>\n",
       "      <td>76</td>\n",
       "      <td>1409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Michael Maxwell</td>\n",
       "      <td>82</td>\n",
       "      <td>4446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sean Brown</td>\n",
       "      <td>93</td>\n",
       "      <td>3354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Mary Adams</td>\n",
       "      <td>93</td>\n",
       "      <td>3801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>George Wagner</td>\n",
       "      <td>81</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Joshua Harris</td>\n",
       "      <td>78</td>\n",
       "      <td>3381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Mark Harris</td>\n",
       "      <td>84</td>\n",
       "      <td>3717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>George King</td>\n",
       "      <td>90</td>\n",
       "      <td>3740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Tanya Diaz</td>\n",
       "      <td>92</td>\n",
       "      <td>2153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Kyle Cruz</td>\n",
       "      <td>91</td>\n",
       "      <td>2822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Troy Davis</td>\n",
       "      <td>70</td>\n",
       "      <td>3463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>John Smith</td>\n",
       "      <td>69</td>\n",
       "      <td>2962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         patient_name  avg_heartrate  count\n",
       "0       Anthony Perez             75   2850\n",
       "1   Dr. Amanda Baxter             75   4640\n",
       "2    Melissa Martinez             75   2487\n",
       "3          Crystal Ho             75   2973\n",
       "4     Autumn Calderon             75   1819\n",
       "5       Sharon Brewer             86   1819\n",
       "6    Rachel Contreras             86   4423\n",
       "7       Valerie Reese             86   4326\n",
       "8      Joshua Perkins             83   3484\n",
       "9      Valerie Garcia             83   2726\n",
       "10      Samuel Hughes             83   4138\n",
       "11     Caitlin Garcia             80   3322\n",
       "12    Gabriela Gibson             80   3280\n",
       "13      Sarah Kennedy             80   4661\n",
       "14     Ashley Schmidt             79   3234\n",
       "15       Lynn Russell             79   3716\n",
       "16       Ronald Smith             79   2942\n",
       "17    Michelle Carter             85   3514\n",
       "18    Nicholas Spears             85   3134\n",
       "19         John Estes             85   3264\n",
       "20  William Gomez Jr.            104   1069\n",
       "21     Robert Vincent            100   3171\n",
       "22   Cynthia Figueroa             76   1409\n",
       "23    Michael Maxwell             82   4446\n",
       "24         Sean Brown             93   3354\n",
       "25         Mary Adams             93   3801\n",
       "26      George Wagner             81    685\n",
       "27      Joshua Harris             78   3381\n",
       "28        Mark Harris             84   3717\n",
       "29        George King             90   3740\n",
       "30         Tanya Diaz             92   2153\n",
       "31          Kyle Cruz             91   2822\n",
       "32         Troy Davis             70   3463\n",
       "33         John Smith             69   2962"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wait_until_stream_is_ready(gold_query, 1)\n",
    "\n",
    "df_fact_heartbeat_by_patient_gold.select(\"patient_name\", \"avg_heartrate\", \"count\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32813a55-21b4-4f35-b33a-c62b27f96503",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (3.12-env)",
   "language": "python",
   "name": "3.12-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b62c1ab5-c3fd-48c1-bc20-f4bd5a1729d5",
   "metadata": {},
   "source": [
    "## Demo 3: PySpark Medallion Architecture - Persisted using Parquet and Queried with Spark-SQL\n",
    "### Overview\n",
    "Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive. You can use the Dataset/DataFrame API in Scala, Java, Python or R to express streaming aggregations, event-time windows, stream-to-batch joins, etc. The computation is executed on the same optimized Spark SQL engine. Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs. In short, Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.\n",
    "\n",
    "Internally, by default, Structured Streaming queries are processed using a micro-batch processing engine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees. However, since Spark 2.3, we have introduced a new low-latency processing mode called Continuous Processing, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees. Without changing the Dataset/DataFrame operations in your queries, you will be able to choose the mode based on your application requirements.\n",
    "\n",
    "Having gained a better understanding of how to perform **incremental data processing** in a previous demonstration, where we combined Structured Streaming APIs and Spark SQL, we can now explore the tight integration between Structured Streaming and Delta Lake.\n",
    "\n",
    "#### Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "* Describe Bronze, Silver, and Gold tables\n",
    "* Create a Delta Lake multi-hop pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06b99fd-8ecf-41b9-96a9-a411b73dc124",
   "metadata": {},
   "source": [
    "#### Incremental Updates in the Lakehouse\n",
    "Delta Lake allows users to easily combine streaming and batch workloads in a unified multi-stage pipeline, wherein each stage of the pipeline represents a state of our data valuable to driving core use cases within the business. With all data and metadata resident in object storage in the cloud, multiple users and applications can access data in near-real time, allowing analysts to access the freshest data as it's being processed.\n",
    "\n",
    "![](https://files.training.databricks.com/images/sslh/multi-hop-simple.png)\n",
    "\n",
    "- **Bronze** tables contain raw data ingested from various sources (JSON files, RDBMS data,  IoT data, to name a few examples).\n",
    "- **Silver** tables provide a more refined view of our data. We can join fields from various bronze tables to enrich streaming records, or update account statuses based on recent activity.\n",
    "- **Gold** tables provide business level aggregates often used for reporting and dashboarding. This would include aggregations such as daily active website users, weekly sales per store, or gross revenue per quarter by department. \n",
    "\n",
    "The end outputs are actionable insights, dashboards and reports of business metrics.  By considering our business logic at all steps of the ETL pipeline, we can ensure that storage and compute costs are optimized by reducing unnecessary duplication of data and limiting ad hoc querying against full historic data.  Each stage can be configured as a batch or streaming job, and ACID transactions ensure that we succeed or fail completely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb73f2b-4bfb-42c9-8c23-332470c571a7",
   "metadata": {},
   "source": [
    "### Lab Details:\n",
    "This lab will demonstrate ingesting artificially generated medical data, in JSON format, that simulates heart rate monitor signals captured from numerous devices; therefore, this data represents what would be expected from a Streaming data source.\n",
    "\n",
    "#### Datasets Used:\n",
    "The schema of our two datasets is represented below. Note that we will be manipulating these schema during various steps.\n",
    "\n",
    "##### Recordings:\n",
    "The main dataset uses heart rate recordings from medical devices delivered in the JSON format.\n",
    "\n",
    "| Field | Type |\n",
    "| --- | --- |\n",
    "| device_id | int |\n",
    "| mrn | long |\n",
    "| time | double |\n",
    "| heartrate | double |\n",
    "\n",
    "##### Personally Identifiable Information (PII):\n",
    "These data will later be joined with a static table of patient information stored in an external system to identify patients.\n",
    "\n",
    "| Field | Type |\n",
    "| --- | --- |\n",
    "| mrn | long |\n",
    "| name | string |\n",
    "\n",
    "### 1.0. Prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af67835d-0dd5-461c-9f66-ad60bc27ff74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark-3.5.4-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b270a6-c516-4e23-8b99-2575f3880165",
   "metadata": {},
   "source": [
    "#### 1.1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2254708f-fdba-42af-ae1f-add9b66b07a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6732d0-0f54-4d36-b205-1cc4d2ad831b",
   "metadata": {},
   "source": [
    "#### 1.2. Instantiate Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1450fbb2-eb89-470e-89e9-c69218446ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = os.path.join(os.getcwd(), 'lab_data')\n",
    "data_dir = os.path.join(base_dir, 'healthcare')\n",
    "batch_dir = os.path.join(data_dir, 'batch')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "tracker_stream_dir = os.path.join(stream_dir, 'tracker')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Directory Structure for Data Lakehouse Files\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"healthcare_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database)\n",
    "\n",
    "output_bronze = os.path.join(database_dir, 'bronze')\n",
    "output_silver = os.path.join(database_dir, 'silver')\n",
    "output_gold = os.path.join(database_dir, 'gold')\n",
    "\n",
    "patient_output_bronze = os.path.join(output_bronze, 'dim_patient')\n",
    "heartbeat_output_bronze = os.path.join(output_bronze, 'fact_heartbeat')\n",
    "heartbeat_output_silver = os.path.join(output_silver, 'fact_heartbeat')\n",
    "heartbeat_output_gold = os.path.join(output_gold, 'fact_heartbeat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0183e2c-3f02-4540-9635-4718f5fc28bb",
   "metadata": {},
   "source": [
    "#### 1.3. Define Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c98b7829-d285-4013-896e-9dd329bcb8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_directory_tree(path: str):\n",
    "    '''If it exists, remove the entire contents of a directory structure at a given 'path' parameter's location.'''\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' has been removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4695da09-b727-4763-b75d-a5ec4c9da951",
   "metadata": {},
   "source": [
    "#### 1.4. Create a New Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b9db63-a330-4e9c-897f-40765e5d0724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://JT-5570:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[10]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Parquet Data Lakehouse Heartrate Monitor</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x24a1452c1d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "shuffle_partitions = int(os.cpu_count())\n",
    "\n",
    "sparkConf = SparkConf().setAppName('PySpark Parquet Data Lakehouse Heartrate Monitor')\\\n",
    "    .setMaster(worker_threads)\\\n",
    "    .set('spark.driver.memory', '4g') \\\n",
    "    .set('spark.executor.memory', '2g')\\\n",
    "    .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .set('spark.sql.shuffle.partitions', shuffle_partitions) \\\n",
    "    .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .set(\"spark.sql.streaming.schemaInference\", \"true\") \\\n",
    "    .set('spark.sql.warehouse.dir', database_dir) \\\n",
    "    .set('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420bd759-d44e-477b-b149-1c8229c2394f",
   "metadata": {},
   "source": [
    "#### 1.5. Initialize Data Lakehouse Directory Structure\n",
    "Remove the Data Lakehouse Database Directory Structure to Ensure Idempotency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3dfd2bc-a351-4490-8d77-232165b78f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Directory 'C:\\\\Users\\\\jtupi\\\\Documents\\\\UVA\\\\DS-2002-Teacher\\\\04-PySpark\\\\spark-warehouse\\\\healthcare_dlh' has been removed successfully.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_directory_tree(database_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b55ad1-194e-4796-bdcc-dc1b00a99336",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 2.0. Create Bronze Layer\n",
    "#### 2.1. Read a Batch of Patient dimension data from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2c1ef5f-f581-42c7-8520-1dd980df764b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtupi\\Documents\\UVA\\DS-2002-Teacher\\04-PySpark\\lab_data\\healthcare\\batch\\patient_info.csv\n"
     ]
    }
   ],
   "source": [
    "patient_csv = os.path.join(batch_dir, 'patient_info.csv')\n",
    "print(patient_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3e2bf80-8f26-4378-88a3-0743c92dea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patient = spark.read.format('csv').options(header='true', inferSchema='true').load(patient_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4d740c9-d6de-4229-86a0-9b44390884fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mrn: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "The 'df_patients' table contains 34 rows.\n",
      "+--------+---------------+\n",
      "|     mrn|           name|\n",
      "+--------+---------------+\n",
      "|23940128| Caitlin Garcia|\n",
      "|18064290|  Anthony Perez|\n",
      "|95384990|     Tanya Diaz|\n",
      "|53057176|Autumn Calderon|\n",
      "|96005424|   Ronald Smith|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unit Test ---------------------------------------------------------\n",
    "df_patient.printSchema()\n",
    "print(f\"The 'df_patients' table contains {df_patient.count()} rows.\")\n",
    "df_patient.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c05b1-9a87-4889-a689-e3a2110ee822",
   "metadata": {},
   "source": [
    "#### 2.2. Persist the Patient dimension data to a Parquet file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42643108-ce6e-474e-b537-00f2232077fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patient.write.mode(\"overwrite\").parquet(patient_output_bronze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d1d66-6e3c-486c-a8e6-25553bc17eac",
   "metadata": {},
   "source": [
    "#### 2.3. Read Bronze Patient data into a Temporary View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b92f00e8-b2fd-4c2d-8ded-7bb2c74dcdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(patient_output_bronze).createOrReplaceTempView(\"dim_patients_tempvw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb17eb17-ff6c-43f7-a742-21656b5f64a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|     mrn|           name|\n",
      "+--------+---------------+\n",
      "|23940128| Caitlin Garcia|\n",
      "|18064290|  Anthony Perez|\n",
      "|95384990|     Tanya Diaz|\n",
      "|53057176|Autumn Calderon|\n",
      "|96005424|   Ronald Smith|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unit Test -------------------------------------------\n",
    "spark.sql(\"SELECT * FROM dim_patients_tempvw LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e49c8e-4c34-4c5d-89d9-6f0ed8deec02",
   "metadata": {},
   "source": [
    "#### 2.4. Use Structured Streaming to Read Heartrate Monitor data\n",
    "##### 2.4.1. Read data from a series of JSON source files into an In-Memory Streaming DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b75f7db-aabe-404e-a558-6d0921e8cfa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tracker = (spark.readStream \\\n",
    "             .option(\"schemaLocation\", heartbeat_output_bronze) \\\n",
    "             .option(\"maxFilesPerTrigger\", 1) \\\n",
    "             .option(\"multiLine\", \"true\") \\\n",
    "             .json(tracker_stream_dir)\n",
    "            )\n",
    "\n",
    "df_tracker.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea4387-4e0a-4f61-9c59-2e8878128d58",
   "metadata": {},
   "source": [
    "##### 2.4.2. Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a626f964-9fc1-4eb1-bbde-fa1adc644968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- heartrate: string (nullable = true)\n",
      " |-- mrn: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tracker.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dabadb8-9f56-416a-8d83-404a14a2a30f",
   "metadata": {},
   "source": [
    "#### 2.5. Write Data from Streaming DataFrame into a Parquet output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "469ab7ae-400d-4684-bb59-0ec6e3d84589",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_checkpoint_bronze = os.path.join(heartbeat_output_bronze, '_checkpoint')\n",
    "\n",
    "bronze_query = (df_tracker.writeStream \\\n",
    "                .format(\"parquet\") \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .queryName(\"heartbeat_tracker\")\n",
    "                .trigger(availableNow = True) \\\n",
    "                .option(\"checkpointLocation\", tracker_checkpoint_bronze) \\\n",
    "                .option(\"compression\", \"snappy\") \\\n",
    "                .start(heartbeat_output_bronze)\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce4b3d0-25a6-4a20-b34c-7cee918c9117",
   "metadata": {},
   "source": [
    "##### 2.5.1. Monitor Query Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28498f0c-bfc0-49de-aa00-0a0fb676a143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: fcc99b9d-0f85-466a-9caf-d036b5fbc588\n",
      "Query Name: heartbeat_tracker\n",
      "Query Status: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
      "Last Progress: {'id': 'fcc99b9d-0f85-466a-9caf-d036b5fbc588', 'runId': '4512b489-ba8f-4396-93ee-7d8e7133131a', 'name': 'heartbeat_tracker', 'timestamp': '2025-03-17T16:10:50.242Z', 'batchId': 1, 'numInputRows': 8294, 'inputRowsPerSecond': 5771.746694502435, 'processedRowsPerSecond': 8228.174603174602, 'durationMs': {'addBatch': 424, 'commitOffsets': 174, 'getBatch': 20, 'latestOffset': 185, 'queryPlanning': 12, 'triggerExecution': 1008, 'walCommit': 188}, 'stateOperators': [], 'sources': [{'description': 'FileStreamSource[file:/C:/Users/jtupi/Documents/UVA/DS-2002-Teacher/04-PySpark/lab_data/healthcare/streaming/tracker]', 'startOffset': {'logOffset': 0}, 'endOffset': {'logOffset': 1}, 'latestOffset': None, 'numInputRows': 8294, 'inputRowsPerSecond': 5771.746694502435, 'processedRowsPerSecond': 8228.174603174602}], 'sink': {'description': 'FileSink[C:\\\\Users\\\\jtupi\\\\Documents\\\\UVA\\\\DS-2002-Teacher\\\\04-PySpark\\\\spark-warehouse\\\\healthcare_dlh\\\\bronze\\\\fact_heartbeat]', 'numOutputRows': -1}}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query ID: {bronze_query.id}\")\n",
    "print(f\"Query Name: {bronze_query.name}\")\n",
    "print(f\"Query Status: {bronze_query.status}\")\n",
    "print(f\"Last Progress: {bronze_query.lastProgress}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81c4de4d-d924-47fc-adbb-459728592518",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bbe9b7-c5b3-4450-803f-291fcb6449b6",
   "metadata": {},
   "source": [
    "### 3.0. Create Silver Layer\n",
    "#### 3.1. Read Bronze Streaming Data into a Temporary View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7876734-2e10-4a11-9a1f-1c819087f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(heartbeat_output_bronze).createOrReplaceTempView(\"fact_heartbeat_bronze_tempvw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a517d8b-48b2-43f3-9255-80396f23cfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------+------------------+\n",
      "|device_id|    heartrate|     mrn|              time|\n",
      "+---------+-------------+--------+------------------+\n",
      "|       24|44.4530588466|53057176|1601510459.7677453|\n",
      "|       31|68.7408647233|70379340|1601510873.6363325|\n",
      "|       13|61.5566117017|55527081|1601510999.0770276|\n",
      "|        9| 47.628456003|15902097| 1601511002.711981|\n",
      "|       15|53.2287944137|97376381|1601511277.3706536|\n",
      "+---------+-------------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unit Test ---------------------------------------------------\n",
    "spark.sql(\"SELECT * FROM fact_heartbeat_bronze_tempvw LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70064bd9-f881-4549-9700-545cf2534195",
   "metadata": {},
   "source": [
    "#### 3.2. Persist the Silver table to the Lakehouse\n",
    "##### 3.2.1. Define Silver Query to Join Streaming with Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5e5c138-6cdc-4fc5-a60b-6c789b1b266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_query = \"\"\"\n",
    "    SELECT CAST(h.device_id AS int) AS device_id\n",
    "        , CAST(h.heartrate AS double) AS heartrate\n",
    "        , CAST(h.mrn AS long) AS mrn\n",
    "        , CAST((h.time/1000000) AS timestamp) AS datetime\n",
    "        , p.name\n",
    "    FROM fact_heartbeat_bronze_tempvw AS h\n",
    "    INNER JOIN dim_patients_tempvw AS p\n",
    "    ON CAST(p.mrn AS int) = h.mrn\n",
    "    \"\"\"\n",
    "spark.sql(silver_query).write.mode(\"overwrite\").parquet(heartbeat_output_silver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab1f48-0275-488d-8dfa-de4f9065decf",
   "metadata": {},
   "source": [
    "### 4.0. Create Gold Layer\n",
    "#### 4.1. Read Silver Streaming Data into a Temporary View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d007b59f-c011-4dd9-b311-03dcb8bb5d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(heartbeat_output_silver).createOrReplaceTempView(\"fact_heartbeat_silver_tempvw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61d9d5f4-5950-4c96-b8c0-adfce219f4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------+--------------------+--------------+\n",
      "|device_id|    heartrate|     mrn|            datetime|          name|\n",
      "+---------+-------------+--------+--------------------+--------------+\n",
      "|       35|54.0083778414|27831169|1969-12-31 19:26:...|Ashley Schmidt|\n",
      "|       14|59.7276487851|84682617|1969-12-31 19:26:...|     Kyle Cruz|\n",
      "|        7|55.5989222294|41675882|1969-12-31 19:26:...|    Crystal Ho|\n",
      "|        6|53.1187476559|88104185|1969-12-31 19:26:...| George Wagner|\n",
      "|       10|49.0628332598|18064290|1969-12-31 19:26:...| Anthony Perez|\n",
      "+---------+-------------+--------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unit Test ----------------------------------------------------\n",
    "spark.sql(\"SELECT * FROM fact_heartbeat_silver_tempvw\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252ba18-b5b3-4ea0-9782-bb069705ea9a",
   "metadata": {},
   "source": [
    "#### 4.2. Persist the Gold table to the Lakehouse\n",
    "##### 4.2.1. Define Gold Query to Perform an Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4fb2472-d2b2-4280-b7d9-b02f79dcfe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_query = \"\"\"\n",
    "    SELECT name AS patient_name\n",
    "        , COUNT(device_id) AS total_recordings\n",
    "        , CEILING(AVG(heartrate)) AS avg_heartrate\n",
    "    FROM fact_heartbeat_silver_tempvw\n",
    "    GROUP BY patient_name\n",
    "    ORDER BY avg_heartrate DESC\n",
    "    \"\"\"\n",
    "spark.sql(gold_query).write.mode(\"overwrite\").parquet(heartbeat_output_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4538fd-1f43-4dd8-a31e-c9f4feada940",
   "metadata": {},
   "source": [
    "##### 4.2.2. Read Gold Streaming data into a Temporary View of the Gold table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a36d72b9-1471-4ba5-9b84-48b8f8ae1de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(heartbeat_output_gold).createOrReplaceTempView(\"fact_heartbeat_gold_tempvw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b86060f-962a-4fbd-88c6-dc3650228978",
   "metadata": {},
   "source": [
    "#### 4.3. Display the Gold table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c902220-2af2-4eb7-b6e2-7a913732b389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_name</th>\n",
       "      <th>total_recordings</th>\n",
       "      <th>avg_heartrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anthony Perez</td>\n",
       "      <td>2850</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dr. Amanda Baxter</td>\n",
       "      <td>4640</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Melissa Martinez</td>\n",
       "      <td>2487</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crystal Ho</td>\n",
       "      <td>2973</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Autumn Calderon</td>\n",
       "      <td>1819</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        patient_name  total_recordings  avg_heartrate\n",
       "0      Anthony Perez              2850             75\n",
       "1  Dr. Amanda Baxter              4640             75\n",
       "2   Melissa Martinez              2487             75\n",
       "3         Crystal Ho              2973             75\n",
       "4    Autumn Calderon              1819             75"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_query = \"\"\"\n",
    "    SELECT patient_name\n",
    "        , total_recordings\n",
    "        , avg_heartrate\n",
    "    FROM fact_heartbeat_gold_tempvw\n",
    "    \"\"\"\n",
    "spark.sql(report_query).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32813a55-21b4-4f35-b33a-c62b27f96503",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (3.12-env)",
   "language": "python",
   "name": "3.12-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e4eb82e-6626-4f2d-a4a3-f4b4d7bf2265",
   "metadata": {},
   "source": [
    "### Lab 5: Processing Incremental Updates with PySpark Structured Streaming and Delta tables\n",
    "In this lab you'll apply your knowledge of PySpark and structured streaming to implement a simple multi-hop (medallion) architecture.\n",
    "\n",
    "#### 1.0. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49db51-9b2b-4a76-a6a8-a0c81a910fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4cb0a2-a6e0-4645-a4cd-c7fd5aa2458b",
   "metadata": {},
   "source": [
    "#### 2.0. Instantiate Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b48bd-6d42-44ee-8f3e-f760be729f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = os.path.join(os.getcwd(), 'lab_data')\n",
    "data_dir = os.path.join(base_dir, 'retail-org')\n",
    "customers_stream_dir = os.path.join(data_dir, 'customers')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Directory Structure for Data Lakehouse Files\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"customers_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database)\n",
    "\n",
    "customers_output_bronze = os.path.join(database_dir, 'customers_bronze')\n",
    "customers_output_silver = os.path.join(database_dir, 'customers_silver')\n",
    "customers_output_gold = os.path.join(database_dir, 'customers_gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f96fc91-2301-484f-8221-5d1f020b556a",
   "metadata": {},
   "source": [
    "#### 3.0. Define Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ea358-3f4c-45ed-834e-e04eaf8b5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_directory_tree(path: str):\n",
    "    '''If it exists, remove the entire contents of a directory structure at a given 'path' parameter's location.'''\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' has been removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b153a81-c29c-4d19-9166-93c52bec80c0",
   "metadata": {},
   "source": [
    "#### 4.0. Create a New Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03439397-c71e-475e-969f-bc8fcf1a0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "shuffle_partitions = int(os.cpu_count())\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder \\\n",
    "    .appName('PySpark Customers Delta Table in Juptyer')\\\n",
    "    .master(worker_threads)\\\n",
    "    .config('spark.executor.memory', '2g')\\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\") \\\n",
    "    .config('spark.sql.shuffle.partitions', shuffle_partitions) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", \"true\") \\\n",
    "    .config('spark.sql.warehouse.dir', database_dir)\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb88351-4088-4bc5-9261-d17020bd1a69",
   "metadata": {},
   "source": [
    "### 5.0. Initialize Data Lakehouse Directory Structure\n",
    "Remove the Data Lakehouse Database Directory Structure to Ensure Idempotency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c0a53-9940-41b6-9f3b-159fce1f3c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory_tree(database_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a740574-00a0-4f0e-94f7-a2e682836fa1",
   "metadata": {},
   "source": [
    "#### 6.0. Bronze Table: Ingest and Stage Data\n",
    "This lab uses a collection of customer-related CSV data found in *`../04-PySpark/lab_data/retail-org/customers/`*. \n",
    "<br>This is available to you by way of the `customers_stream_dir` variable.\n",
    "\n",
    "##### 6.1. Read this data into a Stream using schema inference\n",
    "- Use a **`_checkpoint`** folder and the **`schemaLocation`** option to store the schema info in a dedicated folder for **`customers`**.\n",
    "- Set the **`maxFilesPerTrigger`** option to **`1`**.\n",
    "- Set the **`inferSchema`** and **`header`** options to **`true`**.\n",
    "- Use **`.csv()`** to specify the source directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac7fb54-d655-4b5f-8af3-571dd4cc65fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_checkpoint_bronze = os.path.join(customers_output_bronze, '_checkpoint')\n",
    "\n",
    "df_customers_bronze = (\n",
    "    spark.readStream \\\n",
    "    # TODO: Configurations\n",
    ")\n",
    "\n",
    "df_customers_bronze.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0867da-7a59-4c59-ae54-7500b6716dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ce9616-7a4f-4599-9415-27c5148bd437",
   "metadata": {},
   "source": [
    "##### 6.2. Stream the raw data to a Delta table.\n",
    " - Use the **`delta`** format.\n",
    " - Use the **`append`** output mode.\n",
    " - Use **`customers_bronze`** as the **`queryName`**.\n",
    " - Use **`availableNow = True`** for the **`trigger`**\n",
    " - Use the **`_checkpoint`** folder with the **`checkpointLocation`** option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83953cd-473b-458d-a291-fa4c643ba95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_bronze_query = (\n",
    "    df_customers_bronze \\\n",
    "    .writeStream \\\n",
    "    # TODO: Configurations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13f595e-6f24-4bf2-9539-9c3ebffe8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207737cb-bce8-4dd1-97f9-fcb7433e2a07",
   "metadata": {},
   "source": [
    "##### 6.3. Create a Streaming Temporary View named **`customers_bronze_temp`**\n",
    "- Use the **`delta`** format.\n",
    "- Set the **`inferSchema`** option to **`true`**\n",
    "- Load the data from the output of the **`bronze`** delta table (**`customers_output_bronze`**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaf84fe-cb95-47b5-af23-d976884eb1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.readStream \\\n",
    "    # TODO: Configurations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb56a9-294f-4ecc-8c99-97504cd94e76",
   "metadata": {},
   "source": [
    "##### 6.4. Clean and Enhance the Data\n",
    "Use the CTAS syntax to define a new streaming view called **`bronze_enhanced_temp`** that does the following:\n",
    "* Omits records with a null **`postcode`** (set to zero)\n",
    "* Inserts a column called **`receipt_time`** containing a current timestamp using the **`current_timestamp()`** function.\n",
    "* Inserts a column called **`source_file`** containing the input filename using the **`imput_file_name()`** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd45bf-3b1f-4095-a1e4-0c6e42a959b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_bronze_temp = \"\"\"\n",
    "   #TODO: author SQL Statement\n",
    "\"\"\"\n",
    "spark.sql(sql_bronze_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cbb435-80aa-4861-a7b9-e8c4c7110512",
   "metadata": {},
   "source": [
    "#### 7.0. Silver Table\n",
    "##### 7.1. Stream the data from **`bronze_enhanced_temp`** to a **`Delta`** table named **`customers_silver`**.\n",
    " - Use the **`append`** output mode.\n",
    " - Use **`customers_silver`** as the **`queryName`**.\n",
    " - Use **`availableNow = True`** for the **`trigger`**\n",
    " - Use a **`_checkpoint`** folder with the **`checkpointLocation`** option to store the schema info in a dedicated folder for **`customers`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2269fde-af1d-47fd-947d-591f9d1a6083",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_checkpoint_silver = os.path.join(customers_output_silver, '_checkpoint')\n",
    "\n",
    "customers_silver_query = \\\n",
    "(spark.table(\"bronze_enhanced_temp\") \\\n",
    "     .writeStream \\\n",
    "     # TODO: Configurations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36e583-5852-453c-a550-6afb3bcb8f08",
   "metadata": {},
   "source": [
    "##### 7.2. Create a Streaming Temporary View\n",
    "- Create another streaming temporary view named **`customers_silver_temp`** for the **`customers_silver`** table so we can perform business-level queries using SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14241fe9-3949-4c74-8659-54562d38bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.readStream \\\n",
    "     # TODO: Confgurations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218a890d-2d46-4d9b-a1f3-e017fe5482a6",
   "metadata": {},
   "source": [
    "#### 8.0. Gold Table\n",
    "##### 8.1. Use the CTAS syntax to define a new streaming view called **`customer_count_by_state_temp`** that does the following:\n",
    "- Reads data from the **`customers_silver_temp`** temporary view created in the preceding step.\n",
    "- Selects the **`state`** along with the number of customers per (grouped by) state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a981dcc-966e-4c9c-a064-aafde4d47f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_gold_temp = \"\"\"\n",
    "    TODO: Author SQL statement\n",
    "\"\"\"\n",
    "spark.sql(sql_gold_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b01970d-07ac-41ab-a7d8-f1ebe00ddd48",
   "metadata": {},
   "source": [
    "##### 8.2. Stream the data from the **`customer_count_by_state_temp`** view to a Delta table called **`customer_count_by_state_gold`**.\n",
    "- Use the **`complete`** output mode because aggregations like **`count()`** and sorting cannot operate on *unbounded* datasets.  \n",
    "- Use a **`_checkpoint`** folder with the **`checkpointLocation`** option and a dedicated folder for **`customers`** as the checkpoint path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208a889-449f-43ad-af0b-526e29df0dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_count_checkpoint_gold = os.path.join(customers_output_gold, '_checkpoint')\n",
    "\n",
    "customer_count_by_state_gold_query = \\\n",
    "(spark.table(\"customer_count_by_state_temp\") \\\n",
    "     # TODO: Configurations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d70171-85e5-4811-be9a-390c708ed36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_count_by_state_gold_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9b5a3b-47ce-4436-b090-c081e749250c",
   "metadata": {},
   "source": [
    "#### 9.0. Query the Results\n",
    "- Query the **`customer_count_by_state_gold`** table (this will not be a streaming query).\n",
    "- Select the **`state`** and **`customer_count`** columns.\n",
    "- Sort the results by **`customer_count`** in descending order (i.e., from highest to lowest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ca43a-2faf-41b2-8729-6e95db7fd19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_customer_count_query = \"\"\"\n",
    "    TODO: Author SQL query\n",
    "\"\"\"\n",
    "spark.sql(sql_customer_count_query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d980d-76a4-4566-b376-18ab2e2a2640",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (3.12-env)",
   "language": "python",
   "name": "3.12-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

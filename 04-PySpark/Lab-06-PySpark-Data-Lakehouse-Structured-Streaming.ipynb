{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8476b62d-2825-400c-be4c-35bb3aa81b1c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Lab 6: Building a Data Lakehouse with the PySpark Structured Streaming Medallion Architecture\n",
    "This lab will help you learn to use many of the software libraries and programming techniques required to fulfill the requirements of the final end-of-session capstone project for course **DS-2002: Data Systems**. The spirit of the project is to provide a capstone challenge that requires students to demonstrate a practical and functional understanding of each of the data systems and architectural principles covered throughout the session.\n",
    "\n",
    "**These include:**\n",
    "- Relational Database Management Systems (e.g., MySQL, Microsoft SQL Server, Oracle, IBM DB2)\n",
    "  - Online Transaction Processing Systems (OLTP): *Optimized for High-Volume Write Operations; Normalized to 3rd Normal Form.*\n",
    "  - Online Analytical Processing Systems (OLAP): *Optimized for Read/Aggregation Operations; Dimensional Model (i.e, Star Schema)*\n",
    "- NoSQL *(Not Only SQL)* Systems (e.g., MongoDB, CosmosDB, Cassandra, HBase, Redis)\n",
    "- File System *(Data Lake)* Source Systems (e.g., AWS S3, Microsoft Azure Data Lake Storage)\n",
    "  - Various Datafile Formats (e.g., JSON, CSV, Parquet, Text, Binary)\n",
    "- Massively Parallel Processing *(MPP)* Data Integration Systems (e.g., Apache Spark/PySpark, Databricks)\n",
    "- Data Integration Patterns (e.g., Extract-Transform-Load, Extract-Load-Transform, Extract-Load-Transform-Load, Lambda & Kappa Architectures)\n",
    "\n",
    "## Section I: Prerequisites\n",
    "\n",
    "### 1.0. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16355324-fc8a-45b5-b9e5-7b6e8ac814c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import certifi\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window as W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c2fdf8-2c35-4152-b60a-5e0ae632f60f",
   "metadata": {},
   "source": [
    "### 2.0. Instantiate Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977209d2-77d8-40c6-a497-c5c0958c19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify MySQL Server Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mysql_args = {\n",
    "    \"host_name\" : \"localhost\",\n",
    "    \"port\" : \"3306\",\n",
    "    \"db_name\" : \"northwind\",\n",
    "    \"conn_props\" : {\n",
    "        \"user\" : \"jtupitza\",\n",
    "        \"password\" : \"Passw0rd123!\",\n",
    "        \"driver\" : \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify MongoDB Cluster Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mongodb_args = {\n",
    "    \"cluster_location\" : \"local\", # \"atlas\"\n",
    "    \"user_name\" : \"jtupitza\",\n",
    "    \"password\" : \"Passw0rd1234\",\n",
    "    \"cluster_name\" : \"sandbox\",\n",
    "    \"cluster_subnet\" : \"zibbf\",\n",
    "    \"db_name\" : \"northwind\",\n",
    "    \"collection\" : \"\",\n",
    "    \"null_column_threshold\" : 0.5\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = os.path.join(os.getcwd(), 'lab_data')\n",
    "data_dir = os.path.join(base_dir, 'northwind')\n",
    "batch_dir = os.path.join(data_dir, 'batch')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "\n",
    "orders_stream_dir = os.path.join(stream_dir, 'orders')\n",
    "purchase_orders_stream_dir = os.path.join(stream_dir, 'purchase_orders')\n",
    "inventory_trans_stream_dir = os.path.join(stream_dir, 'inventory_transactions')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Directory Structure for Data Lakehouse Files\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"northwind_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "dest_database_dir = f\"{dest_database}.db\"\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database_dir)\n",
    "\n",
    "orders_output_bronze = os.path.join(database_dir, 'fact_orders', 'bronze')\n",
    "orders_output_silver = os.path.join(database_dir, 'fact_orders', 'silver')\n",
    "orders_output_gold = os.path.join(database_dir, 'fact_orders', 'gold')\n",
    "\n",
    "purchase_orders_output_bronze = os.path.join(database_dir, 'fact_purchase_orders', 'bronze')\n",
    "purchase_orders_output_silver = os.path.join(database_dir, 'fact_purchase_orders', 'silver')\n",
    "purchase_orders_output_gold = os.path.join(database_dir, 'fact_purchase_orders', 'gold')\n",
    "\n",
    "inventory_trans_output_bronze = os.path.join(database_dir, 'fact_inventory_transactions', 'bronze')\n",
    "inventory_trans_output_silver = os.path.join(database_dir, 'fact_inventory_transactions', 'silver')\n",
    "inventory_trans_output_gold = os.path.join(database_dir, 'fact_inventory_transactions', 'gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a5185-ad2c-4612-9498-0e2ff4431c5b",
   "metadata": {},
   "source": [
    "### 3.0. Define Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fd4b85-6029-439d-a66a-1cab0a9aa760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_info(path: str):\n",
    "    file_sizes = []\n",
    "    modification_times = []\n",
    "\n",
    "    '''Fetch each item in the directory, and filter out any directories.'''\n",
    "    items = os.listdir(path)\n",
    "    files = sorted([item for item in items if os.path.isfile(os.path.join(path, item))])\n",
    "\n",
    "    '''Populate lists with the Size and Last Modification DateTime for each file in the directory.'''\n",
    "    for file in files:\n",
    "        file_sizes.append(os.path.getsize(os.path.join(path, file)))\n",
    "        modification_times.append(pd.to_datetime(os.path.getmtime(os.path.join(path, file)), unit='s'))\n",
    "\n",
    "    data = list(zip(files, file_sizes, modification_times))\n",
    "    column_names = ['name','size','modification_time']\n",
    "    \n",
    "    return pd.DataFrame(data=data, columns=column_names)\n",
    "\n",
    "\n",
    "def wait_until_stream_is_ready(query, min_batches=1):\n",
    "    while len(query.recentProgress) < min_batches:\n",
    "        time.sleep(5)\n",
    "        \n",
    "    print(f\"The stream has processed {len(query.recentProgress)} batchs\")\n",
    "\n",
    "\n",
    "def remove_directory_tree(path: str):\n",
    "    '''If it exists, remove the entire contents of a directory structure at a given 'path' parameter's location.'''\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' has been removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "        \n",
    "\n",
    "def drop_null_columns(df, threshold):\n",
    "    '''Drop Columns having a percentage of NULL values that exceeds the given 'threshold' parameter value.'''\n",
    "    columns_with_nulls = [col for col in df.columns if df.filter(df[col].isNull()).count() / df.count() > threshold] \n",
    "    df_dropped = df.drop(*columns_with_nulls) \n",
    "    \n",
    "    return df_dropped\n",
    "    \n",
    "    \n",
    "def get_mysql_dataframe(spark_session, sql_query : str, **args):\n",
    "    '''Create a JDBC URL to the MySQL Database'''\n",
    "    jdbc_url = f\"jdbc:mysql://{args['host_name']}:{args['port']}/{args['db_name']}\"\n",
    "    \n",
    "    '''Invoke the spark.read.format(\"jdbc\") function to query the database, and fill a DataFrame.'''\n",
    "    dframe = spark_session.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"driver\", args['conn_props']['driver']) \\\n",
    "    .option(\"user\", args['conn_props']['user']) \\\n",
    "    .option(\"password\", args['conn_props']['password']) \\\n",
    "    .option(\"query\", sql_query) \\\n",
    "    .load()\n",
    "    \n",
    "    return dframe\n",
    "    \n",
    "\n",
    "def get_mongo_uri(**args):\n",
    "    '''Validate proper input'''\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the 'cluster_location' parameter.\")\n",
    "        \n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        uri = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "        uri += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net/\"\n",
    "    else:\n",
    "        uri = \"mongodb://localhost:27017/\"\n",
    "\n",
    "    return uri\n",
    "\n",
    "\n",
    "def get_spark_conf_args(spark_jars : list, **args):\n",
    "    jars = \"\"\n",
    "    for jar in spark_jars:\n",
    "        jars += f\"{jar}, \"\n",
    "    \n",
    "    sparkConf_args = {\n",
    "        \"app_name\" : \"PySpark Northwind Data Lakehouse (Medallion Architecture)\",\n",
    "        \"worker_threads\" : f\"local[{int(os.cpu_count()/2)}]\",\n",
    "        \"shuffle_partitions\" : int(os.cpu_count()),\n",
    "        \"mongo_uri\" : get_mongo_uri(**args),\n",
    "        \"spark_jars\" : jars[0:-2],\n",
    "        \"database_dir\" : sql_warehouse_dir\n",
    "    }\n",
    "    \n",
    "    return sparkConf_args\n",
    "    \n",
    "\n",
    "def get_spark_conf(**args):\n",
    "    sparkConf = SparkConf().setAppName(args['app_name'])\\\n",
    "    .setMaster(args['worker_threads']) \\\n",
    "    .set('spark.driver.memory', '4g') \\\n",
    "    .set('spark.executor.memory', '2g') \\\n",
    "    .set('spark.jars', args['spark_jars']) \\\n",
    "    .set('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .set('spark.mongodb.input.uri', args['mongo_uri']) \\\n",
    "    .set('spark.mongodb.output.uri', args['mongo_uri']) \\\n",
    "    .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .set('spark.sql.debug.maxToStringFields', 35) \\\n",
    "    .set('spark.sql.shuffle.partitions', args['shuffle_partitions']) \\\n",
    "    .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .set('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .set('spark.sql.warehouse.dir', args['database_dir']) \\\n",
    "    .set('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "    \n",
    "    return sparkConf\n",
    "\n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    '''Get MongoDB Client Connection'''\n",
    "    mongo_uri = get_mongo_uri(**args)\n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        client = pymongo.MongoClient(mongo_uri, tlsCAFile=certifi.where())\n",
    "\n",
    "    elif args['cluster_location'] == \"local\":\n",
    "        client = pymongo.MongoClient(mongo_uri)\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"A MongoDB Client could not be created.\")\n",
    "\n",
    "    return client\n",
    "    \n",
    "    \n",
    "# TODO: Rewrite this to leverage PySpark?\n",
    "def set_mongo_collections(mongo_client, db_name : str, data_directory : str, json_files : list):\n",
    "    db = mongo_client[db_name]\n",
    "    \n",
    "    for file in json_files:\n",
    "        db.drop_collection(file)\n",
    "        json_file = os.path.join(data_directory, json_files[file])\n",
    "        with open(json_file, 'r') as openfile:\n",
    "            json_object = json.load(openfile)\n",
    "            file = db[file]\n",
    "            result = file.insert_many(json_object)\n",
    "        \n",
    "    mongo_client.close()\n",
    "    \n",
    "\n",
    "def get_mongodb_dataframe(spark_session, **args):\n",
    "    '''Query MongoDB, and create a DataFrame'''\n",
    "    dframe = spark_session.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "        .option(\"database\", args['db_name']) \\\n",
    "        .option(\"collection\", args['collection']).load()\n",
    "\n",
    "    '''Drop the '_id' index column to clean up the response.'''\n",
    "    dframe = dframe.drop('_id')\n",
    "    \n",
    "    '''Call the drop_null_columns() function passing in the dataframe.'''\n",
    "    dframe = drop_null_columns(dframe, args['null_column_threshold'])\n",
    "    \n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c929bad4-705f-4a01-8d9d-bba2d84d115c",
   "metadata": {},
   "source": [
    "### 4.0. Initialize Data Lakehouse Directory Structure\n",
    "Remove the Data Lakehouse Database Directory Structure to Ensure Idempotency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09080f-afdc-4969-86cc-0fbd0835faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory_tree(database_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa65216-97fc-48d9-b19e-a1f342c53155",
   "metadata": {},
   "source": [
    "### 5.0. Create a New Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a416acc5-71ca-40f5-986a-7eeac55b6317",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "\n",
    "jars = []\n",
    "mysql_spark_jar = os.path.join(os.getcwd(), \"mysql-connector-j-9.1.0\", \"mysql-connector-j-9.1.0.jar\")\n",
    "mssql_spark_jar = os.path.join(os.getcwd(), \"sqljdbc_12.8\", \"enu\", \"jars\", \"mssql-jdbc-12.8.1.jre11.jar\")\n",
    "\n",
    "jars.append(mysql_spark_jar)\n",
    "#jars.append(mssql_spark_jar)\n",
    "\n",
    "sparkConf_args = get_spark_conf_args(jars, **mongodb_args)\n",
    "\n",
    "sparkConf = get_spark_conf(**sparkConf_args)\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8701c-0120-4ba2-81cc-e53ba1f08651",
   "metadata": {},
   "source": [
    "### 6.0. Create a New Metadata Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d76e2-3c95-4d0e-9063-37344aae674c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {dest_database} CASCADE;\")\n",
    "\n",
    "sql_create_db = f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {dest_database}\n",
    "    COMMENT 'DS-2002 Lab 06 Database'\n",
    "    WITH DBPROPERTIES (contains_pii = true, purpose = 'DS-2002 Lab 6.0');\n",
    "\"\"\"\n",
    "spark.sql(sql_create_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ae5f1c-7ec7-408c-bc89-822b7a75b859",
   "metadata": {},
   "source": [
    "## Section II: Populate Dimensions by Ingesting \"Cold-path\" Reference Data \n",
    "### 1.0. Fetch Data from the File System\n",
    "#### 1.1. Verify the location of the source data files on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3629bc-e7ec-4ed4-9de7-17f648691e4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_file_info(batch_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e13a92a-2e5e-46b3-babc-ac10f20d35da",
   "metadata": {},
   "source": [
    "#### 1.2. Populate the <span style=\"color:darkred\">Employees Dimension</span>\n",
    "##### 1.2.1. Use PySpark to Read data from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f9eac0-5c92-4a98-8455-8be2a034d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_csv = os.path.join(batch_dir, 'northwind_employees.csv')\n",
    "print(employee_csv)\n",
    "\n",
    "df_dim_employees = spark.read.format('csv').options(header='true', inferSchema='true').load(employee_csv)\n",
    "df_dim_employees.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e9aedf-9033-403d-a802-b09be878865f",
   "metadata": {},
   "source": [
    "##### 1.2.2. Make Necessary Transformations to the New DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18421cba-8e15-44b0-8328-0ff1f1968936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'id' column to 'employee_id' ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_employees = df_dim_employees.withColumnRenamed(\"id\", \"employee_id\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_employees.createOrReplaceTempView(\"employees\")\n",
    "sql_employees = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY employee_id) AS employee_key\n",
    "    FROM employees;\n",
    "\"\"\"\n",
    "df_dim_employees = spark.sql(sql_employees)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "ordered_columns = ['employee_key', 'employee_id', 'first_name', 'last_name'\n",
    "                   , 'company', 'job_title', 'business_phone', 'home_phone', 'fax_number'\n",
    "                   , 'address', 'city', 'state_province', 'zip_postal_code', 'country_region']\n",
    "\n",
    "df_dim_employees = df_dim_employees[ordered_columns]\n",
    "df_dim_employees.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a826b-085a-42ce-b9b1-f0991e578eb6",
   "metadata": {},
   "source": [
    "##### 1.2.3. Save as the <span style=\"color:darkred\">dim_employees</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931ed64-99f7-4281-b903-a052c89ce6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_employees.write.saveAsTable(f\"{dest_database}.dim_employees\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7eacbe-1cad-4ded-9b98-814d140ed8d6",
   "metadata": {},
   "source": [
    "##### 1.2.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f418e-b9aa-4ef0-a256-b7dd3de1519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_employees;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_employees LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebca6e1-cf05-44bc-b2c4-29ea6492d5c3",
   "metadata": {},
   "source": [
    "#### 1.3. Populate the <span style=\"color:darkred\">Shippers Dimension</span>\n",
    "##### 1.3.1. Use PySpark to Read Data from a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f5587-5c93-409f-ba2d-d60883a82c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1). Get a reference to the 'northwind_shippers.csv' file.\n",
    "\n",
    "# 2). Use Spark to read the CSV file data into the 'df_dim_shippers' variable.\n",
    "#     Remember to specify that the first row contains column names (header), and to infer the schema.\n",
    "\n",
    "# 3). Unit Test: Convert the spark dataframe to a Pandas dataframe, and display the first two rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce5dcc-59df-4f7e-abf2-604668f77ed4",
   "metadata": {},
   "source": [
    "##### 1.3.2 Make Necessary Transformations to the New DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7498239a-2320-4496-b594-18fb5fe6279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'id' column to 'shipper_id' ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c325c-2fe6-4c8e-b8c2-aeefb127c3ab",
   "metadata": {},
   "source": [
    "##### 1.3.3. Save as the <span style=\"color:darkred\">dim_shippers</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d688f917-d9df-4bee-bd94-1a6147a5558b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17cb59e9-b2a7-4cd7-b601-9e99857af89e",
   "metadata": {},
   "source": [
    "##### 1.3.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73177f0-7590-491e-ae7e-d5256a11dc33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54c1854d-8ea5-421d-b2d7-62109d67581f",
   "metadata": {},
   "source": [
    "### 2.0. Fetch Reference Data from a MongoDB Atlas Database\n",
    "#### 2.1. Create a New MongoDB Database, and Load Each JSON File into a New MongoDB Collection\n",
    "**NOTE:** The following cell **can** be run more than once because the **set_mongo_collection()** function **is** idempotent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ae8b9-128b-49c6-9aeb-532dafdafb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_mongo_client(**mongodb_args)\n",
    "\n",
    "json_files = {\"customers\" : \"northwind_customers.json\",\n",
    "              \"invoices\" : 'northwind_invoices.json',\n",
    "              \"suppliers\" : 'northwind_suppliers.json'\n",
    "             }\n",
    "\n",
    "set_mongo_collections(client, mongodb_args[\"db_name\"], batch_dir, json_files) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1565e612-daeb-45f0-a743-e05b3a137c6e",
   "metadata": {},
   "source": [
    "#### 2.2. Populate the <span style=\"color:darkred\">Customers Dimension</span>\n",
    "##### 2.2.1. Fetch Data from the New MongoDB <span style=\"color:darkred\">Customers</span> Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b966f7-d6e4-4f51-81b8-871099605278",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongodb_args[\"collection\"] = \"customers\"\n",
    "\n",
    "df_dim_customers = get_mongodb_dataframe(spark, **mongodb_args)\n",
    "df_dim_customers.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59029949-2103-40b8-8393-3707e0e83846",
   "metadata": {},
   "source": [
    "##### 2.2.2. Make Necessary Transformations to the New Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30819428-fcf7-4668-b080-b124da524417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'id' column to 'customer_id' ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using the SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c71d4b7-b620-49db-ba43-0b5c83b77b61",
   "metadata": {},
   "source": [
    "##### 2.2.3. Save as the <span style=\"color:darkred\">dim_customers</span> table in the Data lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3467a6d-8037-4d25-b9c1-18e9bbc413c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27d8d82a-dba4-40e3-aecc-0f4763b94679",
   "metadata": {},
   "source": [
    "##### 2.2.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64add729-9a97-483c-874e-c5aede8eadc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16e3947d-c807-4bdb-8552-bb6ec0389574",
   "metadata": {},
   "source": [
    "#### 2.4. Populate the <span style=\"color:darkred\">Suppliers Dimension</span>\n",
    "##### 2.3.1. Fetch Data from the New MongoDB <span style=\"color:darkred\">Suppliers</span> Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c122223-74ee-4dd2-b506-8e6fcd85b028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fcc6f32-aecc-475d-8b37-f775b89f9356",
   "metadata": {},
   "source": [
    "##### 2.3.2. Make Necessary Transformations to the New Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb757e10-4c6f-4210-b9ce-29894a1d30dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'id' column to 'supplier_id' ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e1dde-546d-40d9-9c2d-dd56ea7aad03",
   "metadata": {},
   "source": [
    "##### 2.3.3. Save as the <span style=\"color:darkred\">dim_suppliers</span> table in the Data lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e4322-4e8a-4279-92b8-0eaa18ee871e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b525fa75-7fca-445f-a586-5d8acf995f6e",
   "metadata": {},
   "source": [
    "##### 2.3.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f390f44-e113-49f3-8ac7-8c864bed2ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a11cf43d-c4be-446a-bdf9-73400fab4561",
   "metadata": {},
   "source": [
    "#### 2.4. Populate the <span style=\"color:darkred\">Invoices Dimension</span>\n",
    "##### 2.4.1. Fetch Data from the New MongoDB <span style=\"color:darkred\">Invoices</span> Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f01d2-1a8e-445a-9913-549b06c783ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe339fa3-907d-4e5a-9f72-3cd71e179a84",
   "metadata": {},
   "source": [
    "##### 2.4.2. Make Necessary Transformations to the New Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7dc00b-1241-4463-9d07-35066896cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'id' column to 'invoice_id' ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c3fc36-487a-4342-b47f-c2cbc04f27b4",
   "metadata": {},
   "source": [
    "##### 2.4.3. Save as the <span style=\"color:darkred\">dim_invoices</span> table in the Data lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76518f9d-133a-4c8a-969b-6ed014fd028f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46e9d072-764d-478e-80d7-f5b84f6618b3",
   "metadata": {},
   "source": [
    "##### 2.4.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b046075-3b8f-45fb-a5ce-235862429603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8947777-e4a6-461f-9490-301ccaf4b06a",
   "metadata": {},
   "source": [
    "### 3.0. Fetch Reference Data from a MySQL Database\n",
    "#### 3.1. Populate the <span style=\"color:darkred\">Date Dimension</span>\n",
    "##### 3.1.1 Fetch data from the <span style=\"color:darkred\">dim_date</span> table in MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4af01d-9f24-4287-87cd-07fc31502958",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dim_date = f\"SELECT * FROM {mysql_args['db_name']}.dim_date\"\n",
    "df_dim_date = get_mysql_dataframe(spark, sql_dim_date, **mysql_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be56fc-b6e3-42e1-801d-3474957aee4a",
   "metadata": {},
   "source": [
    "##### 3.1.2. Save as the <span style=\"color:darkred\">dim_date</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868fc437-5f13-447e-8def-1fc663eb05a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_date.write.saveAsTable(f\"{dest_database}.dim_date\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f64095-6b91-43b4-b96f-20d4fafa9a35",
   "metadata": {},
   "source": [
    "##### 3.1.3. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f8ad8-b44d-4a82-bd4e-d94757964c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_date;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_date LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df0a18f-6a72-4b02-b4a7-f41080d0ca4d",
   "metadata": {},
   "source": [
    "#### 3.2. Populate the <span style=\"color:darkred\">Product Dimension</span>\n",
    "##### 3.2.1. Fetch data from the <span style=\"color:darkred\">Products</span> table in MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff551005-e91d-4dd4-9b2a-36326b0d73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using the SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b715baa-4e46-40dd-9cd1-eeafbaefa977",
   "metadata": {},
   "source": [
    "##### 3.2.2. Perform any Necessary Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6d060-81e9-4929-b41a-a8444ec62405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'id' column to 'product_id' \n",
    "# ----------------------------------------------------------------------------------\n",
    "# Using the monotonically_increasing_id() function has some limitations: starts with zero (0), and is not sequential.\n",
    "    # df_dim_products = df_dim_products.withColumn(\"product_key\", monotonically_increasing_id())\n",
    "df_dim_products = df_dim_products.withColumnRenamed(\"id\", \"product_id\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Drop unwanted columns (description and attachments)\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e24d3a-7dfd-45d6-ae1e-a5023ed4e420",
   "metadata": {},
   "source": [
    "##### 3.2.3. Save as the <span style=\"color:darkred\">dim_products</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76683fe-db95-4ab7-934e-e3d28bc4e717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6942bdac-df24-4e80-bd1b-e1dc276d2c34",
   "metadata": {},
   "source": [
    "##### 3.2.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a723db4-148f-4a75-a40c-dac99aab3559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "375bf727-5f3e-4589-a4da-d57446a376f3",
   "metadata": {},
   "source": [
    "### 4.0. Verify Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29795432-f447-4c0b-a5a1-cc7d4b4fb008",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"USE {dest_database};\")\n",
    "spark.sql(\"SHOW TABLES\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f158f3-15d4-4b77-8080-c3d1bc68986a",
   "metadata": {},
   "source": [
    "## Section III: Integrate Reference Data with Real-Time Data\n",
    "### 6.0. Use PySpark Structured Streaming to Process (Hot Path) <span style=\"color:darkred\">Orders</span> Fact Data  \n",
    "#### 6.1. Verify the location of the source data files on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b506c14-a897-4ac8-a01e-590ddc2e49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file_info(orders_stream_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725f2bda-d78e-4731-9d4e-10d75d3607ba",
   "metadata": {},
   "source": [
    "#### 6.2. Create the Bronze Layer: Stage <span style=\"color:darkred\">Orders Fact table</span> Data\n",
    "##### 6.2.1. Read \"Raw\" JSON file data into a Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe375157-dfad-4442-8563-0a05cacecb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_bronze = (\n",
    "    spark.readStream \\\n",
    "    .option(\"schemaLocation\", orders_output_bronze) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .json(orders_stream_dir)\n",
    ")\n",
    "\n",
    "df_orders_bronze.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e94035a-3bc4-4ba2-abf5-63ca89b4e0f7",
   "metadata": {},
   "source": [
    "##### 6.2.2. Write the Streaming Data to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd60124-791d-4df3-99eb-a5943c5b3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_checkpoint_bronze = os.path.join(orders_output_bronze, '_checkpoint')\n",
    "\n",
    "orders_bronze_query = (\n",
    "    df_orders_bronze\n",
    "    # Add Current Timestamp and Input Filename columns for Traceability\n",
    "    .withColumn(\"receipt_time\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    \n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"orders_bronze\")\n",
    "    .trigger(availableNow = True) \\\n",
    "    .option(\"checkpointLocation\", orders_checkpoint_bronze) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(orders_output_bronze)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b461d-1d9c-4233-aae9-cae37539c0ba",
   "metadata": {},
   "source": [
    "##### 6.2.3. Unit Test: Implement Query Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299479ba-fb32-4260-8585-14f9c47b1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query ID: {orders_bronze_query.id}\")\n",
    "print(f\"Query Name: {orders_bronze_query.name}\")\n",
    "print(f\"Query Status: {orders_bronze_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc62d72c-f857-43c0-8c37-716ef3501064",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1baf025-d6fd-4b69-89d2-dd89c3205496",
   "metadata": {},
   "source": [
    "#### 6.3. Create the Silver Layer: Integrate \"Cold-path\" Data & Make Transformations\n",
    "##### 6.3.1. Prepare Role-Playing Dimension Primary and Business Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b1f33c-0ebd-4156-9f14-e8588cab2c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_order_date = df_dim_date.select(col(\"date_key\").alias(\"order_date_key\"), col(\"full_date\").alias(\"order_full_date\"))\n",
    "df_dim_paid_date = df_dim_date.select(col(\"date_key\").alias(\"paid_date_key\"), col(\"full_date\").alias(\"paid_full_date\"))\n",
    "df_dim_shipped_date = df_dim_date.select(col(\"date_key\").alias(\"shipped_date_key\"), col(\"full_date\").alias(\"shipped_full_date\"))\n",
    "df_dim_shippers = df_dim_shippers.withColumnRenamed(\"shipper_id\", \"shipper_no\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b0437-66a7-4386-9084-4594faab6df0",
   "metadata": {},
   "source": [
    "##### 6.3.2. Define Silver Query to Join Streaming with Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ea470-0cc3-4530-99de-8cf5beae64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_silver = spark.readStream.format(\"parquet\").load(orders_output_bronze) \\\n",
    "    .join(df_dim_customers, \"customer_id\") \\\n",
    "    .join(df_dim_employees, \"employee_id\") \\\n",
    "    .join(df_dim_products, \"product_id\") \\\n",
    "    .join(df_dim_shippers, df_dim_shippers.shipper_no == col(\"shipper_id\").cast(IntegerType()), \"left_outer\") \\\n",
    "    .join(df_dim_order_date, df_dim_order_date.order_full_date.cast(DateType()) == col(\"order_date\").cast(DateType()), \"inner\") \\\n",
    "    .join(df_dim_shipped_date, df_dim_shipped_date.shipped_full_date.cast(DateType()) == col(\"shipped_date\").cast(DateType()), \"left_outer\") \\\n",
    "    .join(df_dim_paid_date, df_dim_paid_date.paid_full_date.cast(DateType()) == col(\"paid_date\").cast(DateType()), \"left_outer\") \\\n",
    "    .select(col(\"order_id\").cast(LongType()), \\\n",
    "            col(\"order_detail_id\").cast(LongType()), \\\n",
    "            df_dim_customers.customer_key.cast(LongType()), \\\n",
    "            df_dim_employees.employee_key.cast(LongType()), \\\n",
    "            df_dim_products.product_key.cast(LongType()), \\\n",
    "            df_dim_shippers.shipper_key.cast(IntegerType()), \\\n",
    "            df_dim_order_date.order_date_key.cast(LongType()), \\\n",
    "            df_dim_paid_date.paid_date_key.cast(LongType()), \\\n",
    "            df_dim_shipped_date.shipped_date_key.cast(LongType()), \\\n",
    "            col(\"quantity\"), \\\n",
    "            col(\"unit_price\"), \\\n",
    "            col(\"discount\"), \\\n",
    "            col(\"shipping_fee\"), \\\n",
    "            col(\"taxes\"), \\\n",
    "            col(\"tax_rate\"), \\\n",
    "            col(\"payment_type\"), \\\n",
    "            col(\"order_status\"), \\\n",
    "            col(\"order_details_status\") \\\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce36782-37f7-4a7a-baf8-bc6c9d7e647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_silver.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcb25db-964b-41ed-bfc8-9ca633c219b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_silver.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6c9fd4-c595-4c38-adb3-c5a3a1f13f53",
   "metadata": {},
   "source": [
    "##### 6.3.3. Write the Transformed Streaming data to the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b60d0-0985-4641-822a-7e7995ce0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_checkpoint_silver = os.path.join(orders_output_silver, '_checkpoint')\n",
    "\n",
    "orders_silver_query = (\n",
    "    df_orders_silver.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"orders_silver\")\n",
    "    .trigger(availableNow = True) \\\n",
    "    .option(\"checkpointLocation\", orders_checkpoint_silver) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(orders_output_silver)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a6c445-16dd-4752-9e46-6ba7e9cef121",
   "metadata": {},
   "source": [
    "##### 6.3.4. Unit Test: Implement Query Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e7be2-fea6-4578-9237-344983c52111",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query ID: {orders_silver_query.id}\")\n",
    "print(f\"Query Name: {orders_silver_query.name}\")\n",
    "print(f\"Query Status: {orders_silver_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493145c-19b3-4446-84e1-b9a37ecf2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_silver_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb44a5a-b470-4928-bc73-b5ed9ff8da74",
   "metadata": {},
   "source": [
    "#### 6.4. Create Gold Layer: Perform Aggregations\n",
    "##### 6.4.1. Define a Query to Create a Business Report\n",
    "Create a new Gold table using the PySpark API. The table should include the number of Products sold per Category each Month. The results should include The Month, Product Category and Number of Products sold, sorted by the month number when the orders were placed: e.g., January, February, March."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0bb272-a7c1-4f2a-a3d9-5ee5fd6f454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_by_product_category_gold = spark.readStream.format(\"parquet\").load(orders_output_silver) \\\n",
    ".join(df_dim_products, \"product_key\") \\\n",
    ".join(df_dim_date, df_dim_date.date_key.cast(IntegerType()) == col(\"order_date_key\").cast(IntegerType())) \\\n",
    ".groupBy(\"month_of_year\", \"category\", \"month_name\") \\\n",
    ".agg(count(\"product_key\").alias(\"product_count\")) \\\n",
    ".orderBy(asc(\"month_of_year\"), desc(\"product_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36930416-660d-4814-a66c-a08d732ba921",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_by_product_category_gold.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb8abc8-c889-4ed2-8d37-f15bb1c27ffb",
   "metadata": {},
   "source": [
    "##### 6.4.2. Write the Streaming data to a Parquet File in \"Complete\" mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0619456b-2217-4a27-b9cf-5d764610a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_gold_query = (\n",
    "    df_orders_by_product_category_gold.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"fact_orders_by_product_category\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c1d0b-8341-4c5a-9970-c7e2f444feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_until_stream_is_ready(orders_gold_query, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627bd3d-d17b-4ab2-b62a-1b988d10a7c6",
   "metadata": {},
   "source": [
    "##### 6.4.3. Query the Gold Data from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d4cf7a-de59-4596-a8af-0e84790d7b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_orders_by_product_category = spark.sql(\"SELECT * FROM fact_orders_by_product_category\")\n",
    "df_fact_orders_by_product_category.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c34a4-74ab-45cb-b096-fdd860a00012",
   "metadata": {},
   "source": [
    "##### 6.4.4 Create the Final Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ba5a02-ea03-4f9e-99d8-f50327779467",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_orders_by_product_category_gold_final = df_fact_orders_by_product_category \\\n",
    ".select(col(\"month_name\").alias(\"Month\"), \\\n",
    "        col(\"category\").alias(\"Product Category\"), \\\n",
    "        col(\"product_count\").alias(\"Product Count\")) \\\n",
    ".orderBy(asc(\"month_of_year\"), desc(\"Product Count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010c836-3593-4ff5-afb1-f5b29798f3dd",
   "metadata": {},
   "source": [
    "##### 6.4.5. Load the Final Results into a New Table and Display the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7ba782-5dca-4030-b386-04e8f7c5e249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fact_orders_by_product_category_gold_final.write.saveAsTable(f\"{dest_database}.fact_orders_by_product_category\", mode=\"overwrite\")\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.fact_orders_by_product_category\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be545e39-4352-4688-b7c1-92097acd2e88",
   "metadata": {},
   "source": [
    "### 7.0. Use PySpark Structured Streaming to Process (Hot Path) <span style=\"color:darkred\">Inventory Transactions</span> Fact Data\n",
    "#### 7.1. Verify the location of the source data files on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb671dd-1607-41d4-8ebd-e1edce43311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file_info(inventory_trans_stream_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4029fc9f-6ed7-48f6-a347-0245926a542f",
   "metadata": {},
   "source": [
    "#### 7.2. Create the Bronze Layer: Stage <span style=\"color:darkred\">Inventory Transactions Fact table</span> Data\n",
    "##### 7.2.1. Read \"Raw\" JSON file data into a Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f047ae2-f893-4dea-884b-f2e5ebdd5253",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inventory_trans_bronze = (\n",
    "    spark.readStream \\\n",
    "    #TODO: load data from 'inventory_trans_stream_dir'\n",
    ")\n",
    "\n",
    "df_inventory_trans_bronze.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739e693f-7dcb-4e8d-ac6a-dd12b19c32ad",
   "metadata": {},
   "source": [
    "##### 7.2.2. Write the Streaming Data to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3598e1e1-3c23-4421-8cd9-da6c93133cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_trans_checkpoint_bronze = os.path.join(inventory_trans_output_bronze, '_checkpoint')\n",
    "\n",
    "inventory_trans_bronze_query = (\n",
    "    df_inventory_trans_bronze\n",
    "    # TODO: Add Current Timestamp and Input Filename columns for Traceability\n",
    "    # TODO: writeStream to 'inventory_trans_output_bronze' in 'append' mode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb998485-8a0f-4ba1-b51c-03836b36f1af",
   "metadata": {},
   "source": [
    "##### 7.2.3. Unit Test: Implement Query Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b12a2c9-d8fb-4df0-b146-46ce1a1b1eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query ID: {inventory_trans_bronze_query.id}\")\n",
    "print(f\"Query Name: {inventory_trans_bronze_query.name}\")\n",
    "print(f\"Query Status: {inventory_trans_bronze_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d3f5e-c44e-47f2-9f59-cffcd37ad64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_trans_bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8794a1-1b49-4c01-a87c-8b84cd79c30e",
   "metadata": {},
   "source": [
    "#### 7.3. Create the Silver Layer: Integrate \"Cold-path\" Data & Make Transformations\n",
    "##### 7.3.1. Prepare Role-Playing Dimension Primary and Business Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ce86a8-af37-487c-8b5b-7e70b50cd550",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_created_date = #TODO: Copy df_dim_date and rename 'date_key' and 'full_date' columns.\n",
    "df_dim_modified_date = #TODO: Copy df_dim_date and rename 'date_key' and 'full_date' columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f7397e-1b73-4ca3-b65c-8fa92063676b",
   "metadata": {},
   "source": [
    "##### 7.3.2. Define Silver Query to Join Streaming with Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0721a-016e-4b46-af06-e63b26fc0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inventory_trans_silver = spark.readStream.format(\"parquet\").load(inventory_trans_output_bronze) \\\n",
    "    # .join to the dim_products dimension\n",
    "    # .join to the dim_created_date dimension\n",
    "    # .join to the dim_created_date\n",
    "    # .join to the dim_modified_date dimension\n",
    "    # .select() the appropriate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62550a69-a8b1-40ea-8f99-9e2aeb864d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inventory_trans_silver.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d7610-93db-4864-99ce-b2eb6a3ff27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inventory_trans_silver.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8748cf85-83f5-437c-8888-63cdbc82dfce",
   "metadata": {},
   "source": [
    "##### 7.3.3. Write the Transformed Streaming data to the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ecda0-0f45-41fe-98ad-1f183034743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_trans_checkpoint_silver = os.path.join(inventory_trans_output_silver, '_checkpoint')\n",
    "\n",
    "inventory_trans_silver_query = (\n",
    "    df_inventory_trans_silver.writeStream \\\n",
    "    # TODO: writeStream, in 'parquet' format, to 'inventory_trans_output_silver' in 'append' mode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9465503-b7e8-45bd-9f7a-eff4487470ed",
   "metadata": {},
   "source": [
    "##### 7.3.4. Unit Test: Implement Query Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d0584-5ac9-4ba8-adbd-9a240313460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query ID: {inventory_trans_silver_query.id}\")\n",
    "print(f\"Query Name: {inventory_trans_silver_query.name}\")\n",
    "print(f\"Query Status: {inventory_trans_silver_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5ebfb9-11f5-462f-a8dd-e8d273a78be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_trans_silver_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef4e324-0c76-4cab-9eb2-e87148ba92b8",
   "metadata": {},
   "source": [
    "#### 7.4. Create Gold Layer: Perform Aggregations\n",
    "##### 7.4.1. Define a Query to Create a Business Report\n",
    "Create a new Gold table using the PySpark API. The table should include the total quantity (total quantity) of the inventory transactions placed per Product. Include the Inventory Transaction Type, and the Product Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f7927-50a2-46c2-9b5e-7081e1f3e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_inventory_trans_by_product_gold = spark.readStream.format(\"parquet\").load(inventory_trans_output_silver) \\\n",
    "    #.join to the df_dim_products dimension\n",
    "    #.join to the df_dim_date dimension on the 'created_date_key'\n",
    "    # group by the 'calendar_quarter', 'transaction_type', and 'product_name columns\n",
    "    # sum the 'quantity' column to create the 'Total Quantity' column\n",
    "    # order by the 'Total Quantity' column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf05ceb-f4d6-44d0-b8e9-600a99b15c2d",
   "metadata": {},
   "source": [
    "##### 7.4.2. Write the Streaming data to Memory in \"Complete\" mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d116c-dc4a-42e9-bb0f-4cb7ba4f648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_trans_gold_query = (\n",
    "    df_fact_inventory_trans_by_product_gold.writeStream \\\n",
    "    # create the new \"fact_inventory_trans_by_product\" query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2067cb-8161-431a-aa44-a64aeb4edbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_until_stream_is_ready(inventory_trans_gold_query, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a5435-0180-4155-a4db-3d5ded849b13",
   "metadata": {},
   "source": [
    "##### 7.4.3. Query the Gold Data from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93253cef-fb6a-4fa4-b622-db2ff1a13676",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_inventory_trans_by_product = spark.sql(\"SELECT * FROM fact_inventory_trans_by_product\")\n",
    "df_fact_inventory_trans_by_product.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054ebf94-58fe-4851-9399-c1f41b80c556",
   "metadata": {},
   "source": [
    "##### 7.4.4 Create the Final Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4582066-5790-4931-beb7-c90f03ded4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_inventory_trans_by_product_gold_final = df_fact_inventory_trans_by_product \\\n",
    "    # .select() the 'calendar_quarter' column as 'Quarter Created',\n",
    "    # 'transaction_type' as 'Transaction', 'product_name' as 'Product', and 'Total Quantity'\n",
    "    # ordered by 'Total Quantity'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dfd0b7-bba2-40f5-8efd-b8b47961da38",
   "metadata": {},
   "source": [
    "##### 7.4.5. Load the Final Results into a New Table and Display the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252eec1c-abe8-439f-bc4c-2ec181a48baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_inventory_trans_by_product_gold_final.write.saveAsTable(f\"{dest_database}.fact_inventory_trans_by_product\", mode=\"overwrite\")\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.fact_inventory_trans_by_product\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b2dcb2-82e3-4a30-b04d-b06d92044735",
   "metadata": {},
   "source": [
    "### 8.0. Use PySpark Structured Streaming to Process (Hot Path) <span style=\"color:darkred\">Purchase Orders</span> Fact Data\n",
    "#### 8.1. Verify the location of the source data files on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b074ee6-767a-426e-9ff0-466f50f2ed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file_info(purchase_orders_stream_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1743fafa-155b-46be-b28a-1d4b31dd44e3",
   "metadata": {},
   "source": [
    "#### 8.2. Create the Bronze Layer: Stage <span style=\"color:darkred\">Purchase Orders Fact table</span> Data\n",
    "##### 8.2.1. Read \"Raw\" JSON file data into a Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4945ea6-4215-4ccf-9d49-628f3848fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purchase_orders_bronze = (\n",
    "    spark.readStream \\\n",
    "    # TODO: load data from 'purchase_orders_stream_dir'\n",
    ")\n",
    "\n",
    "df_purchase_orders_bronze.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5f3a9-fcc2-485c-96b8-c62ef8bee2cb",
   "metadata": {},
   "source": [
    "##### 8.2.2. Write the Streaming Data to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a81c5-ffb0-4158-8644-67b1e58c55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_orders_checkpoint_bronze = os.path.join(purchase_orders_output_bronze, '_checkpoint')\n",
    "\n",
    "purchase_orders_bronze_query = (\n",
    "    df_purchase_orders_bronze\n",
    "    # TODO: Add Current Timestamp and Input Filename columns for Traceability\n",
    "    # TODO: writeStream to 'purchase_orders_output_bronze' in 'append' mode\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be7eecc-e366-4c42-8ce2-139124e43fbf",
   "metadata": {},
   "source": [
    "##### 8.2.3. Unit Test: Implement Query Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd8e3da-70f4-4132-942e-6b494cd48998",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query ID: {purchase_orders_bronze_query.id}\")\n",
    "print(f\"Query Name: {purchase_orders_bronze_query.name}\")\n",
    "print(f\"Query Status: {purchase_orders_bronze_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac5d2ab-d1fd-4900-a14a-c0605ac58f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_orders_bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c242a939-e121-4d6c-83a5-70817bc6d1b8",
   "metadata": {},
   "source": [
    "#### 8.3. Create the Silver Layer: Integrate \"Cold-path\" Data & Make Transformations\n",
    "##### 8.3.1. Prepare Role-Playing Dimension Primary and Business Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d5b82-da69-471f-a362-21940382adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_created_by = #TODO: Copy 'df_dim_employees' and rename 'employee_key' and 'employee_id' columns.\n",
    "df_dim_approved_by = #TODO: Copy 'df_dim_employees' and rename 'employee_key' and 'employee_id' columns.\n",
    "df_dim_submitted_by = #TODO: Copy 'df_dim_employees' and rename 'employee_key' and 'employee_id' columns.\n",
    "\n",
    "df_dim_submitted_date = #TODO: Copy df_dim_date and rename 'date_key' and 'full_date' columns.\n",
    "df_dim_creation_date = #TODO: Copy df_dim_date and rename 'date_key' and 'full_date' columns.\n",
    "df_dim_approved_date = #TODO: Copy df_dim_date and rename 'date_key' and 'full_date' columns.\n",
    "df_dim_date_received = #TODO: Copy df_dim_date and rename 'date_key' and 'full_date' columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a6681-5eb1-4c45-89b2-fd125a9eb6a0",
   "metadata": {},
   "source": [
    "##### 8.3.2. Define Silver Query to Join Streaming with Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a807bc-186d-44c0-b181-01c2f6c6e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purchase_orders_silver = spark.readStream.format(\"parquet\").load(purchase_orders_output_bronze) \\\n",
    "    # .join 'inner' to the df_dim_products dimension\n",
    "    # .join 'inner' to the df_dim_suppliers\n",
    "    # .join 'left_outer' to the df_dim_created_by dimension\n",
    "    # .join 'left_outer' to the df_dim_approved_by dimension\n",
    "    # .join 'left_outer' to the df_dim_submitted_by dimension\n",
    "    # .join 'inner' to the df_dim_submitted_date dimension\n",
    "    # .join 'inner' to the df_dim_creation_date\n",
    "    # .join 'left_outer' to the df_dim_approved_date dimension\n",
    "    # .join 'left_outer' to the df_dim_date_received dimension\n",
    "    # .select() the appropriate columns from the 'purchase orders bronze' stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ffacdc-813d-49db-bc17-5d510371545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purchase_orders_silver.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7daf6d-3121-4dd7-9e01-4b51397b69cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purchase_orders_silver.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b891abcc-4c45-4364-bd6d-30a0a7d886e6",
   "metadata": {},
   "source": [
    "##### 8.3.3. Write the Transformed Streaming data to the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b414de3c-ab4d-4d4f-a987-81fb417c99a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_orders_checkpoint_silver = os.path.join(purchase_orders_output_silver, '_checkpoint')\n",
    "\n",
    "purchase_orders_silver_query = (\n",
    "    df_purchase_orders_silver.writeStream \\\n",
    "    # TODO: writeStream, in 'parquet' format, to 'purchase_orders_output_silver' in 'append' mode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2133b8c8-59a9-4f0b-a47a-4eb5b6b64c7b",
   "metadata": {},
   "source": [
    "##### 8.3.4. Unit Test: Implement Query Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6eb141-c92e-4f70-9757-0f7e93eac435",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query ID: {purchase_orders_silver_query.id}\")\n",
    "print(f\"Query Name: {purchase_orders_silver_query.name}\")\n",
    "print(f\"Query Status: {purchase_orders_silver_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be4f13-e58e-40dd-8286-a94c195747fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_orders_silver_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e153434-28ad-47f1-889c-d3c5ad6e9825",
   "metadata": {},
   "source": [
    "#### 8.4. Create Gold Layer: Perform Aggregations\n",
    "##### 8.4.1. Define a Query to Create a Business Report\n",
    "Create a new Gold table using the PySpark API. The table should include the Suppliers' Company Name, the Product Name, the Total Quantity, Total Unit Cost, and Total List Price for all the purchase orders placed per Supplier for each Product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a3b8d-7be1-457e-b2b2-0858269a886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_pos_products_per_supplier_gold = spark.readStream.format(\"parquet\").load(purchase_orders_output_silver) \\\n",
    "# .join to the 'df_dim_products' dimension\n",
    "# .join to the 'df_dim_suppliers' dimension\n",
    "# .groupBy 'company' and 'product_name'\n",
    "# sum 'po_detail_quantity' as 'Total Quantity'\n",
    "# sum 'po_detail_unit_cost' as 'Total Unit Cost'\n",
    "# sum 'list_price' as 'Total List Price'\n",
    "# orderBy 'Total Quantity' in descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c972e0-8ac6-458f-b6ab-703348f0b887",
   "metadata": {},
   "source": [
    "##### 8.4.2. Write the Streaming data to Memory in \"Complete\" mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a4ee8-54a1-4f20-b100-0116be5c1bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_orders_gold_query = (\n",
    "    df_fact_pos_products_per_supplier_gold.writeStream \\\n",
    "    # create the new \"fact_pos_products_per_supplier\" query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fe0ce1-73ec-4233-9246-8663086fa37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_until_stream_is_ready(purchase_orders_gold_query, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8969d4cc-85e2-4ddc-bae2-875243fbcd98",
   "metadata": {},
   "source": [
    "##### 8.4.3. Query the Gold Data from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8848aa-adb0-40e6-8c68-de8b2936e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_pos_products_per_supplier = spark.sql(\"SELECT * FROM fact_pos_products_per_supplier\")\n",
    "df_fact_pos_products_per_supplier.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5babafb4-e123-41ff-8dbc-62b29fe58dd6",
   "metadata": {},
   "source": [
    "##### 8.4.4. Create the Final Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb16afc-a048-44b0-b8e3-6a73d677723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_pos_products_per_supplier_gold_final = df_fact_pos_products_per_supplier \\\n",
    "# .select() the 'company' column as 'Supplier', the 'product_name' column as 'Product',\n",
    "# along with the 'Total Quantity', 'Total Unit Cost', and 'Total List Price' columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a3411-c649-4036-a28f-4fdc289c5814",
   "metadata": {},
   "source": [
    "##### 8.4.5. Load the Final Results into a New Table and Display the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45668f44-148b-418f-b956-7dbe2de36e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_pos_products_per_supplier_gold_final.write.saveAsTable(f\"{dest_database}.fact_pos_products_per_supplier\", mode=\"overwrite\")\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.fact_pos_products_per_supplier\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efd14eb-1c30-4d29-b5ca-46dadafa5419",
   "metadata": {},
   "source": [
    "### 9.0. Stop the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6edcbef-aa54-4ca0-aa9f-2b58c8931537",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (3.12-env)",
   "language": "python",
   "name": "3.12-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

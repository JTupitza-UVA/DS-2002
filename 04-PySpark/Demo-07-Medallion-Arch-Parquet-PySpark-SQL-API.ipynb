{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209e78d4-6dc7-4719-b542-88ba738bef17",
   "metadata": {},
   "source": [
    "## Demo: Medallion Architecture Using Parquet Files and Queried with the PySpark SQL API\n",
    "### Overview\n",
    "Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive. You can use the Dataset/DataFrame API in Scala, Java, Python or R to express streaming aggregations, event-time windows, stream-to-batch joins, etc. The computation is executed on the same optimized Spark SQL engine. Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs. In short, Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.\n",
    "\n",
    "Internally, by default, Structured Streaming queries are processed using a micro-batch processing engine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees. However, since Spark 2.3, we have introduced a new low-latency processing mode called Continuous Processing, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees. Without changing the Dataset/DataFrame operations in your queries, you will be able to choose the mode based on your application requirements.\n",
    "\n",
    "### Lab Details:\n",
    "\n",
    "This lab will demonstrate ingesting artificially generated medical data, in JSON format, that simulates heart rate monitor signals captured from numerous devices; therefore, this data represents what would be expected from a Streaming data source.\n",
    "\n",
    "#### Datasets Used:\n",
    "The schema of our two datasets is represented below. Note that we will be manipulating these schema during various steps.\n",
    "\n",
    "##### Recordings:\n",
    "The main dataset uses heart rate recordings from medical devices delivered in the JSON format.\n",
    "\n",
    "| Field | Type |\n",
    "| --- | --- |\n",
    "| device_id | int |\n",
    "| mrn | long |\n",
    "| time | double |\n",
    "| heartrate | double |\n",
    "\n",
    "##### Personally Identifiable Information (PII):\n",
    "These data will later be joined with a static table of patient information stored in an external system to identify patients.\n",
    "\n",
    "| Field | Type |\n",
    "| --- | --- |\n",
    "| mrn | long |\n",
    "| name | string |\n",
    "\n",
    "### Prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af67835d-0dd5-461c-9f66-ad60bc27ff74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark-3.5.4-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b270a6-c516-4e23-8b99-2575f3880165",
   "metadata": {},
   "source": [
    "#### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2254708f-fdba-42af-ae1f-add9b66b07a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6732d0-0f54-4d36-b205-1cc4d2ad831b",
   "metadata": {},
   "source": [
    "#### Instantiate Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1450fbb2-eb89-470e-89e9-c69218446ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = os.path.join(os.getcwd(), 'lab_data')\n",
    "data_dir = os.path.join(base_dir, 'healthcare')\n",
    "batch_dir = os.path.join(data_dir, 'batch')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "tracker_stream_dir = os.path.join(stream_dir, 'tracker')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Directory Structure for Data Lakehouse Files\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"healthcare_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database)\n",
    "\n",
    "patient_output_bronze = os.path.join(database_dir, 'dim_patient')\n",
    "heartbeat_output_bronze = os.path.join(database_dir, 'fact_heartbeat_bronze')\n",
    "heartbeat_output_silver = os.path.join(database_dir, 'fact_heartbeat_silver')\n",
    "heartbeat_output_gold = os.path.join(database_dir, 'fact_heartbeat_gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4695da09-b727-4763-b75d-a5ec4c9da951",
   "metadata": {},
   "source": [
    "#### Create a New Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b9db63-a330-4e9c-897f-40765e5d0724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://JT-5570:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[10]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Heartrate Monitor in Juptyer</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ba7a9dbf80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "shuffle_partitions = int(os.cpu_count())\n",
    "\n",
    "sparkConf = SparkConf().setAppName('PySpark Heartrate Monitor in Juptyer')\\\n",
    "    .setMaster(worker_threads)\\\n",
    "    .set('spark.driver.memory', '2g') \\\n",
    "    .set('spark.executor.memory', '3g')\\\n",
    "    .set(\"spark.sql.adaptive.enabled\", 'false') \\\n",
    "    .set('spark.sql.shuffle.partitions', shuffle_partitions) \\\n",
    "    .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .set('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .set('spark.sql.warehouse.dir', database_dir) \\\n",
    "    .set('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b55ad1-194e-4796-bdcc-dc1b00a99336",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Create Bronze Layer\n",
    "#### Read a Batch of Patient dimension data from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2c1ef5f-f581-42c7-8520-1dd980df764b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtupi\\Documents\\UVA\\DS-2002-Teacher\\04-PySpark\\lab_data\\healthcare\\batch\\patient_info.csv\n"
     ]
    }
   ],
   "source": [
    "patient_csv = os.path.join(batch_dir, 'patient_info.csv')\n",
    "\n",
    "# Unit Test\n",
    "print(patient_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d740c9-d6de-4229-86a0-9b44390884fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mrn: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "The 'df_patients' table contains 34 rows.\n",
      "+--------+---------------+\n",
      "|     mrn|           name|\n",
      "+--------+---------------+\n",
      "|23940128| Caitlin Garcia|\n",
      "|18064290|  Anthony Perez|\n",
      "|95384990|     Tanya Diaz|\n",
      "|53057176|Autumn Calderon|\n",
      "|96005424|   Ronald Smith|\n",
      "+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_patient = spark.read.format('csv').options(header='true', inferSchema=True).load(patient_csv)\n",
    "\n",
    "# Unit Test -------------\n",
    "df_patient.printSchema()\n",
    "print(f\"The 'df_patients' table contains {df_patient.count()} rows.\")\n",
    "df_patient.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1e8d3c-823e-4339-ac75-871f688ab1dc",
   "metadata": {},
   "source": [
    "#### Persist the Patient dimension data to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e391257d-e7eb-44d5-a879-af0c97bbc8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patient.write.mode(\"overwrite\").parquet(patient_output_bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db8e4871-dda4-4f70-b186-fc04838ac11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|     mrn|           name|\n",
      "+--------+---------------+\n",
      "|23940128| Caitlin Garcia|\n",
      "|18064290|  Anthony Perez|\n",
      "|95384990|     Tanya Diaz|\n",
      "|53057176|Autumn Calderon|\n",
      "|96005424|   Ronald Smith|\n",
      "+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unit Test -------------------------------------------\n",
    "df_patient = spark.read.parquet(patient_output_bronze)\n",
    "df_patient.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e49c8e-4c34-4c5d-89d9-6f0ed8deec02",
   "metadata": {},
   "source": [
    "#### Use Structured Streaming to Read Heartrate Monitor data\n",
    "##### Read data from a series of JSON source files into a streaming DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b75f7db-aabe-404e-a558-6d0921e8cfa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tracker = (spark.readStream \\\n",
    "              .option(\"schemaLocation\", heartbeat_output_bronze) \\\n",
    "              .option(\"maxFilesPerTrigger\", 1) \\\n",
    "              .option(\"multiLine\", \"true\") \\\n",
    "              .json(tracker_stream_dir)\n",
    "             )\n",
    "\n",
    "df_tracker.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a626f964-9fc1-4eb1-bbde-fa1adc644968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- heartrate: string (nullable = true)\n",
      " |-- mrn: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unit Test -------------\n",
    "print(type(df_tracker))\n",
    "df_tracker.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dabadb8-9f56-416a-8d83-404a14a2a30f",
   "metadata": {},
   "source": [
    "#### Write data from streaming DataFrame into a Parquet output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "469ab7ae-400d-4684-bb59-0ec6e3d84589",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_checkpoint_bronze = os.path.join(heartbeat_output_bronze, '_checkpoint')\n",
    "\n",
    "bronze_query = (df_tracker.writeStream \\\n",
    "                .format(\"parquet\") \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .queryName(\"heartbeat_tracker_bronze\")\n",
    "                .trigger(availableNow = True) \\\n",
    "                .option(\"checkpointLocation\", tracker_checkpoint_bronze) \\\n",
    "                .option(\"compression\", \"snappy\") \\\n",
    "                .start(heartbeat_output_bronze)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28498f0c-bfc0-49de-aa00-0a0fb676a143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: e47a47f1-6d08-445c-af50-84e0aa0b772d\n",
      "Query Name: heartbeat_tracker_bronze\n",
      "Query Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n",
      "Last Progress: None\n"
     ]
    }
   ],
   "source": [
    "# Unit Test ----------------------------------\n",
    "print(f\"Query ID: {bronze_query.id}\")\n",
    "print(f\"Query Name: {bronze_query.name}\")\n",
    "print(f\"Query Status: {bronze_query.status}\")\n",
    "print(f\"Last Progress: {bronze_query.lastProgress}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ff79c4a-d9df-4863-85b2-aff008b07d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bbe9b7-c5b3-4450-803f-291fcb6449b6",
   "metadata": {},
   "source": [
    "### Create Silver Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70064bd9-f881-4549-9700-545cf2534195",
   "metadata": {},
   "source": [
    "#### Define Silver Query to Join Streaming with Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57ef3f36-cea0-48bc-8c32-3f6cc0d3eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver = spark.readStream.format(\"parquet\").load(heartbeat_output_bronze) \\\n",
    "    .join(df_patient, \"mrn\") \\\n",
    "    .select(col(\"device_id\").cast(IntegerType()), \\\n",
    "            col(\"mrn\").cast(LongType()), \\\n",
    "            (col(\"time\")/1e6).cast(TimestampType()).alias(\"datetime\"), \\\n",
    "            col(\"heartrate\").cast(DoubleType()), \\\n",
    "            col(\"name\").alias(\"patient_name\")\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a80c45b-71ae-4f41-9493-4eb4ae40c615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- mrn: long (nullable = true)\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- heartrate: double (nullable = true)\n",
      " |-- patient_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unit Test -----------\n",
    "df_silver.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf5bff-3d78-43c4-aa62-cfe56dbc2357",
   "metadata": {},
   "source": [
    "#### Persist Data to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8d425ce-63ac-446c-8dc6-1ce7d133069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_checkpoint_silver = os.path.join(heartbeat_output_silver, '_checkpoint')\n",
    "\n",
    "silver_query = (df_silver.writeStream \\\n",
    "                .format(\"parquet\") \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .queryName(\"heartbeat_tracker_silver\")\n",
    "                .trigger(availableNow = True) \\\n",
    "                .option(\"checkpointLocation\", tracker_checkpoint_silver) \\\n",
    "                .option(\"compression\", \"snappy\") \\\n",
    "                .start(heartbeat_output_silver)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66b27cbe-6dd6-438f-9f18-41b593206cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Test -------------------------------------------\n",
    "silver_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab1f48-0275-488d-8dfa-de4f9065decf",
   "metadata": {},
   "source": [
    "### Create Gold Layer\n",
    "##### Define Gold Query to Perform an Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4edc0481-c30f-47a1-b621-1cdd6d8360c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heartrate_by_patient_gold = spark.readStream.format(\"parquet\").load(heartbeat_output_silver) \\\n",
    "    .groupBy('patient_name') \\\n",
    "    .agg((ceiling(avg(\"heartrate\")).alias(\"avg_heartrate\")), \\\n",
    "        (count(\"device_id\").alias(\"count\"))) \\\n",
    "    .orderBy(desc(\"avg_heartrate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c96d533-bbb6-4640-ae82-aaa4e41ecdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- patient_name: string (nullable = true)\n",
      " |-- avg_heartrate: long (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unit Test -----\n",
    "df_heartrate_by_patient_gold.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a6689-7277-46e3-ac86-1f2518643d44",
   "metadata": {},
   "source": [
    "#### Persist Gold Data to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "730bccb7-1a7f-4f94-8a3e-d331753d3a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_query = (df_heartrate_by_patient_gold.writeStream \\\n",
    "              .outputMode(\"complete\") \\\n",
    "              .queryName(\"fact_heartbeat_by_patient_gold\") \\\n",
    "              .format(\"memory\")\n",
    "              .start()\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35262c91-2e04-40d6-8246-a4f4ba8102ec",
   "metadata": {},
   "source": [
    "##### Read Gold Streaming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7f23127-029f-4698-9199-f151e71cb124",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_heartbeat_by_patient_gold = spark.sql(\"SELECT * FROM fact_heartbeat_by_patient_gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40afc6ce-1235-4cde-9839-c5c7f9dcb62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba9e89b-7470-4e50-bcb5-65313109e870",
   "metadata": {},
   "source": [
    "##### Display the Gold table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c902220-2af2-4eb7-b6e2-7a913732b389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_name</th>\n",
       "      <th>avg_heartrate</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [patient_name, avg_heartrate, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fact_heartbeat_by_patient_gold \\\n",
    "    .select(\"patient_name\", \"avg_heartrate\", \"count\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32813a55-21b4-4f35-b33a-c62b27f96503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (3.12-env)",
   "language": "python",
   "name": "3.12-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

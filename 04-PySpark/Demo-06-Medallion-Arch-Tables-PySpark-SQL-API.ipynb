{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209e78d4-6dc7-4719-b542-88ba738bef17",
   "metadata": {},
   "source": [
    "## Demo: Medallion Architecture Using Tables and Queried with the PySpark SQL API\n",
    "### Overview\n",
    "Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive. You can use the Dataset/DataFrame API in Scala, Java, Python or R to express streaming aggregations, event-time windows, stream-to-batch joins, etc. The computation is executed on the same optimized Spark SQL engine. Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs. In short, Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.\n",
    "\n",
    "Internally, by default, Structured Streaming queries are processed using a micro-batch processing engine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees. However, since Spark 2.3, we have introduced a new low-latency processing mode called Continuous Processing, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees. Without changing the Dataset/DataFrame operations in your queries, you will be able to choose the mode based on your application requirements.\n",
    "\n",
    "### Lab Details:\n",
    "\n",
    "This lab will demonstrate ingesting artificially generated medical data, in JSON format, that simulates heart rate monitor signals captured from numerous devices; therefore, this data represents what would be expected from a Streaming data source.\n",
    "\n",
    "#### Datasets Used:\n",
    "The schema of our two datasets is represented below. Note that we will be manipulating these schema during various steps.\n",
    "\n",
    "##### Recordings:\n",
    "The main dataset uses heart rate recordings from medical devices delivered in the JSON format.\n",
    "\n",
    "| Field | Type |\n",
    "| --- | --- |\n",
    "| device_id | int |\n",
    "| mrn | long |\n",
    "| time | double |\n",
    "| heartrate | double |\n",
    "\n",
    "##### Personally Identifiable Information (PII):\n",
    "These data will later be joined with a static table of patient information stored in an external system to identify patients.\n",
    "\n",
    "| Field | Type |\n",
    "| --- | --- |\n",
    "| mrn | long |\n",
    "| name | string |\n",
    "\n",
    "### Prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af67835d-0dd5-461c-9f66-ad60bc27ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b270a6-c516-4e23-8b99-2575f3880165",
   "metadata": {},
   "source": [
    "#### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2254708f-fdba-42af-ae1f-add9b66b07a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6732d0-0f54-4d36-b205-1cc4d2ad831b",
   "metadata": {},
   "source": [
    "#### Instantiate Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1450fbb2-eb89-470e-89e9-c69218446ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = os.path.join(os.getcwd(), 'lab_data')\n",
    "data_dir = os.path.join(base_dir, 'healthcare')\n",
    "batch_dir = os.path.join(data_dir, 'batch')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "tracker_stream_dir = os.path.join(stream_dir, 'tracker')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Directory Structure for Data Lakehouse Files\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"healthcare_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database)\n",
    "\n",
    "patient_output_bronze = os.path.join(database_dir, 'dim_patient')\n",
    "heartbeat_output_bronze = os.path.join(database_dir, 'fact_heartbeat_bronze')\n",
    "heartbeat_output_silver = os.path.join(database_dir, 'fact_heartbeat_silver')\n",
    "heartbeat_output_gold = os.path.join(database_dir, 'fact_heartbeat_gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4695da09-b727-4763-b75d-a5ec4c9da951",
   "metadata": {},
   "source": [
    "#### Create a New Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b9db63-a330-4e9c-897f-40765e5d0724",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "shuffle_partitions = int(os.cpu_count())\n",
    "\n",
    "sparkConf = SparkConf().setAppName('PySpark Heartrate Monitor in Juptyer')\\\n",
    "    .setMaster(worker_threads)\\\n",
    "    .set('spark.driver.memory', '2g') \\\n",
    "    .set('spark.executor.memory', '3g')\\\n",
    "    .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .set('spark.sql.shuffle.partitions', shuffle_partitions) \\\n",
    "    .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .set('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .set('spark.sql.warehouse.dir', sql_warehouse_dir) \\\n",
    "    .set('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b55ad1-194e-4796-bdcc-dc1b00a99336",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Create Bronze Layer\n",
    "#### Read a Batch of Patient dimension data from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c1ef5f-f581-42c7-8520-1dd980df764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_csv = os.path.join(batch_dir, 'patient_info.csv')\n",
    "\n",
    "# Unit Test -------\n",
    "print(patient_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d740c9-d6de-4229-86a0-9b44390884fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patient = spark.read.format('csv').options(header='true', inferSchema='true').load(patient_csv)\n",
    "\n",
    "# Unit Test ----------------------------------------------------------\n",
    "df_patient.printSchema()\n",
    "print(f\"The 'df_patients' table contains {df_patient.count()} rows.\")\n",
    "df_patient.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1e8d3c-823e-4339-ac75-871f688ab1dc",
   "metadata": {},
   "source": [
    "#### Persist the Patient dimension data into a Lakehouse table \n",
    "##### Create a New Data Lakehouse Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d486659c-a1d9-4d75-9dc3-da2c8b062bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE DATABASE {dest_database};\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8f9c85-602e-4efe-924f-dce0daaf47f7",
   "metadata": {},
   "source": [
    "##### Create the 'dim_patients' Dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c7911d-9afd-4862-a1b4-f62f4b11372d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_patient.write.saveAsTable(f\"{dest_database}.dim_patients\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb17eb17-ff6c-43f7-a742-21656b5f64a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Test ------------------------------------------------------------\n",
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_patients;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_patients LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e49c8e-4c34-4c5d-89d9-6f0ed8deec02",
   "metadata": {},
   "source": [
    "#### Use Structured Streaming to Read Heartrate Monitor data\n",
    "##### Read data from a series of JSON source files into a streaming DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75f7db-aabe-404e-a558-6d0921e8cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tracker = (spark.readStream \\\n",
    "             .option(\"schemaLocation\", tracker_output_bronze) \\\n",
    "             .option(\"maxFilesPerTrigger\", 1) \\\n",
    "             .option(\"multiLine\", \"true\") \\\n",
    "             .json(tracker_stream_dir)\n",
    "            )\n",
    "\n",
    "df_tracker.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626f964-9fc1-4eb1-bbde-fa1adc644968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Test -------------\n",
    "print(type(df_tracker))\n",
    "df_tracker.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dabadb8-9f56-416a-8d83-404a14a2a30f",
   "metadata": {},
   "source": [
    "#### Write data from streaming DataFrame into a Streaming Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ab7ae-400d-4684-bb59-0ec6e3d84589",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_checkpoint_bronze = os.path.join(tracker_output_bronze, '_checkpoint')\n",
    "\n",
    "bronze_query = (df_tracker.writeStream \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .queryName(\"heartbeat_tracker_bronze\")\n",
    "                .trigger(availableNow = True) \\\n",
    "                .option(\"checkpointLocation\", tracker_checkpoint_bronze) \\\n",
    "                .option(\"compression\", \"snappy\") \\\n",
    "                .toTable(f\"{dest_database}.fact_heartrate_bronze\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28498f0c-bfc0-49de-aa00-0a0fb676a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Test ----------------------------------\n",
    "print(f\"Query ID: {bronze_query.id}\")\n",
    "print(f\"Query Name: {bronze_query.name}\")\n",
    "print(f\"Query Status: {bronze_query.status}\")\n",
    "print(f\"Last Progress: {bronze_query.lastProgress}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80be4a0-e51d-4b8d-9a39-d214120c255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a398c6-6cff-44ad-8ede-5299f913f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Test ------------------------------------------------------------------ \n",
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.fact_heartrate_bronze;\").show()\n",
    "spark.table(f\"{dest_database}.fact_heartrate_bronze\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bbe9b7-c5b3-4450-803f-291fcb6449b6",
   "metadata": {},
   "source": [
    "### Create Silver Layer\n",
    "#### Define Silver Query to Join Streaming with Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dae414-881b-4da6-a408-fcf21f1f6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver = spark.table(f\"{dest_database}.fact_heartrate_bronze\") \\\n",
    "    .join(df_patient, \"mrn\") \\\n",
    "    .select(col(\"device_id\").cast(IntegerType()), \\\n",
    "            col(\"mrn\").cast(LongType()), \\\n",
    "            (col(\"time\")/1e6).cast(TimestampType()).alias(\"datetime\"), \\\n",
    "            from_unixtime(\"time\", \"MM/dd/yyyy\").alias(\"date\"), \\\n",
    "            from_unixtime(\"time\", \"hh:mm:ss a z\").alias(\"time\"), \\\n",
    "            col(\"heartrate\").cast(DoubleType()), \\\n",
    "            col(\"name\").alias(\"patient_name\")\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561285e-66c6-4545-8b53-33d0880e8f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Test ------------\n",
    "df_silver.printSchema()\n",
    "df_silver.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf5bff-3d78-43c4-aa62-cfe56dbc2357",
   "metadata": {},
   "source": [
    "#### Persist Silver Data to a Table in the Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d425ce-63ac-446c-8dc6-1ce7d133069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_table = f\"{dest_database}.fact_heartrate_silver\"\n",
    "df_silver.write.saveAsTable(silver_table, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728bc58-edd9-47a1-8601-880e6e96d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Test -------------------------------------------\n",
    "spark.sql(f\"DESCRIBE EXTENDED {silver_table};\").show()\n",
    "spark.table(silver_table).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab1f48-0275-488d-8dfa-de4f9065decf",
   "metadata": {},
   "source": [
    "### Create Gold Layer\n",
    "#### Define Gold Query to Perform an Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34246865-8835-4d1e-8128-eacc2fa25606",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gold = df_silver.groupBy(\"patient_name\") \\\n",
    "    .agg((ceiling(avg(\"heartrate\")).alias(\"avg_heartrate\")), \\\n",
    "        (count(\"device_id\").alias(\"count\"))) \\\n",
    "    .orderBy(desc(\"avg_heartrate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c96d533-bbb6-4640-ae82-aaa4e41ecdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Test -----\n",
    "df_gold.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a6689-7277-46e3-ac86-1f2518643d44",
   "metadata": {},
   "source": [
    "#### Persist Gold Data to a Table in the Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f23127-029f-4698-9199-f151e71cb124",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_table = f\"{dest_database}.fact_heartrate_gold\"\n",
    "df_gold.write.saveAsTable(gold_table, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c902220-2af2-4eb7-b6e2-7a913732b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Test ------------------------------------------------------------\n",
    "spark.table(gold_table).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cfedfc-1583-46c4-9287-73a16cd9834d",
   "metadata": {},
   "source": [
    "#### Display the Gold table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39156c97-abac-4d92-a39e-193a541f54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(gold_table) \\\n",
    "    .select(\"patient_name\", \"avg_heartrate\", \"count\") \\\n",
    "    .orderBy(asc(\"avg_heartrate\")).toPandas().()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c05a073-0ad2-4bc0-b27b-3d1a2dd5859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (3.12-env)",
   "language": "python",
   "name": "3.12-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

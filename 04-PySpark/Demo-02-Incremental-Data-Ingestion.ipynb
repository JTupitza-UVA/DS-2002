{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92c968d-afe2-4a9a-a58c-8c65e74a28f0",
   "metadata": {},
   "source": [
    "## Demo 2: Understanding Incremental Data Ingestion with PySpark Structured Streaming\n",
    "Spark Structured Streaming is a feature of Apache Spark that enables for simplified configuration when processing incremental datasets. Historically, streaming big data has been driven by the need to reduce latency for the sake of providing near real-time analytics; however, this lesson focuses more on implementing incremental data processing.\n",
    "\n",
    "While not absolutely required when working with a *data lakehouse*, many enterprise implementations will derive substantial benefit from incremental data processing. Incremental ETL is important since it allows us to deal solely with new data that has been encountered since the last ingestion. Reliably processing only the new data reduces redundant processing and helps enterprises reliably scale data pipelines.\n",
    "\n",
    "Consider the following datasets and use cases:\n",
    "* Data scientists need secure, de-identified, versioned access to frequently updated records in an operational database\n",
    "* Credit card transactions need to be compared to past customer behavior to identify and flag fraud\n",
    "* A multi-national retailer seeks to serve custom product recommendations using purchase history\n",
    "* Log files from distributed systems need to be analayzed to detect and respond to instabilities\n",
    "* Clickstream data from millions of online shoppers needs to be leveraged for A/B testing of UX\n",
    "\n",
    "These are just a small sample of datasets that grow incrementally and infinitely over time.  Here, we demonstrate the basics of using Spark Structured Streaming for incremental data processing.\n",
    "\n",
    "#### Treating Infinite Data as a Table\n",
    "The main benefit of Spark Structured Streaming is that it enables users to interact with ever-growing data sources as if they were simply a static table of records.\n",
    "\n",
    "<img src=\"http://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png\" width=\"800\"/>\n",
    "\n",
    "In the graphic above, a **data stream** describes any data source that grows over time. New data in a data stream might correspond to:\n",
    "* A new JSON log file landing in cloud storage\n",
    "* Updates to a database captured in a CDC feed\n",
    "* Events queued in a pub/sub messaging feed\n",
    "* A CSV file of sales closed the previous day\n",
    "\n",
    "Historically, to update the results of a continuous stream of real-time data, either the entire source dataset had to be completely reprocessed, or custom logic had to be implemented to identify and process only those files or records that had been added since the previous update was executed.  Structured Streaming enables defining a query against the data source to automatically detect new records and propagate them through previously defined logic.\n",
    "\n",
    "#### Basic Concepts\n",
    "- The developer defines an **input table** by configuring a streaming read against a **source**. The syntax for doing this is similar to working with static data.\n",
    "- A **query** is defined against the input table. Both the DataFrames API and Spark SQL can be used to easily define transformations and actions against the input table.\n",
    "- This logical query on the input table generates the **results table**. The results table contains the incremental state information of the stream.\n",
    "- The **output** of a streaming pipeline will persist updates to the results table by writing to an external **sink**. Generally, a sink will be a durable system such as files or a pub/sub messaging bus.\n",
    "- New rows are appended to the input table for each **trigger interval**. These new rows are essentially analogous to micro-batch transactions and will be automatically propagated through the results table to the sink.\n",
    "\n",
    "<img src=\"http://spark.apache.org/docs/latest/img/structured-streaming-model.png\" width=\"800\"/>\n",
    "\n",
    "For more information, see the section in the <a href=\"http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#basic-concepts\" target=\"_blank\">Structured Streaming Programming Guide</a>.\n",
    "\n",
    "#### End-to-End Fault Tolerance\n",
    "Structured Streaming ensures end-to-end exactly-once fault-tolerance guarantees through _checkpointing_ (discussed below) and <a href=\"https://en.wikipedia.org/wiki/Write-ahead_logging\" target=\"_blank\">Write Ahead Logs</a>.  Structured Streaming sources, sinks, and the underlying execution engine work together to track the progress of stream processing. If a failure occurs, the streaming engine attempts to restart and/or reprocess the data. For best practices on recovering from a failed streaming query see <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/production.html#recover-from-query-failures\" target=\"_blank\">docs</a>. Note: This approach _only_ works if the streaming source is replayable; replayable sources include cloud-based object storage and pub/sub messaging services.\n",
    "\n",
    "At a high level, the underlying streaming mechanism relies on a couple approaches:\n",
    "* First, Structured Streaming uses checkpointing and write-ahead logs to record the offset range of data being processed during each trigger interval.\n",
    "* Next, the streaming sinks are designed to be _idempotent_ â€” that is, multiple writes of the same data (as identified by the offset) do _not_ result in duplicates being written to the sink.\n",
    "\n",
    "Taken together, replayable data sources and idempotent sinks allow Structured Streaming to ensure **end-to-end, exactly-once semantics** under any failure condition.\n",
    "\n",
    "#### Unsupported Operations\n",
    "Most operations on a streaming DataFrame are identical to a static DataFrame. There are <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations\" target=\"_blank\">some exceptions to this</a>.  Consider the model of the data as a constantly appending table. Sorting is one of a handful of operations that is either too complex or logically not possible to do when working with streaming data.  While a full discussion of these exceptions is out of scope for this demonstration, note that advanced streaming methods like windowing and watermarking can be used to add additional functionality to incremental workloads.\n",
    "\n",
    "### 1.0. Implementing a Structured Streaming Data Pipeline\n",
    "The first step for any successful data lakehouse implementation is ingesting into a Delta Lake table from a storage location. Historically, ingesting files from a data lake into a database has been a complicated process; however, Spark Structured Streaming provides an easy-to-use mechanism for incrementally and efficiently processing new data files as they arrive in a file storage location. Due to the benefits and scalability that Spark Structured Streaming delivers, its use is a general **best practice** when ingesting data from storage locations like a cloud object storage.\n",
    "\n",
    "Learning objectives include: \n",
    "* Describing the programming model used by Spark Structured Streaming\n",
    "* Describing the requirements for end-to-end fault tolerance\n",
    "* Configuring required options to perform a streaming read on a source\n",
    "* Configuring required options to perform a streaming write to a sink\n",
    "\n",
    "#### Tasks\n",
    "For the sake of illustrating how continuously generated (unbounded) data is incrementally ingested into a data lakehouse, this demo will execute the following job steps: \n",
    "* Execute Spark Structured Streaming code to incrementally ingest data from storage to Delta Lake\n",
    "* Observe what happens when a new file arrives in a directory\n",
    "* Query a table fed by a streaming query\n",
    "\n",
    "#### Data\n",
    "This lab will demonstrate ingesting artificially generated medical data, in JSON format, that simulates heart rate monitor signals captured from numerous devices; therefore, this data represents what would be expected from a *Streaming* data source.\n",
    "\n",
    "| Field | Type |\n",
    "| --- | --- \n",
    "| device_id | int |\n",
    "| mrn | long |\n",
    "| time | double |\n",
    "| heartrate | double |\n",
    "\n",
    "#### 1.1. Import Shared Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ef6bd-05d3-44f5-9438-af57ffd32e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356eee46-3212-4452-873b-003ca83fd15b",
   "metadata": {},
   "source": [
    "#### 1.2. Instantiate Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b30cb7c-ce19-4eff-bea2-ef900617a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = os.path.join(os.getcwd(), 'lab_data')\n",
    "data_dir = os.path.join(base_dir, 'healthcare')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "tracker_source_dir = os.path.join(stream_dir, 'tracker')\n",
    "tracker_files_processed_dir = os.path.join(stream_dir, 'tracker_files_processed')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Directory Structure for Data Lakehouse Files\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"healthcare_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database)\n",
    "\n",
    "tracker_output = os.path.join(database_dir, 'heartbeat_tracker')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c8dd4d-eeb0-4de8-bdce-70feff62c5ea",
   "metadata": {},
   "source": [
    "#### 1.3. Define Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8eeb96-19c6-41ce-b2ff-2c9811a58062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(path: str):\n",
    "    '''If the destination directory doesn't exist then create it'''\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "            new_dir = os.path.split(path)[1]\n",
    "            print(f\"The directory '{new_dir}' has been created successfully.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occured while attempting to create the directory: {e}\"\n",
    "\n",
    "\n",
    "def get_file_info(path: str):\n",
    "    file_sizes = []\n",
    "    modification_times = []\n",
    "\n",
    "    '''Fetch each item in the directory, and filter out any directories.'''\n",
    "    items = os.listdir(path)\n",
    "    files = sorted([item for item in items if os.path.isfile(os.path.join(path, item))]\n",
    "\n",
    "    '''Populate lists with the Size and Last Modification DateTime for each file in the directory.'''\n",
    "    for file in files:\n",
    "        file_sizes.append(os.path.getsize(os.path.join(path, file)))\n",
    "        modification_times.append(pd.to_datetime(os.path.getmtime(os.path.join(path, file)), unit='s'))\n",
    "\n",
    "    data = list(zip(files, file_sizes, modification_times))\n",
    "    column_names = ['name','size','modification_time']\n",
    "    \n",
    "    return pd.DataFrame(data=data, columns=column_names)\n",
    "\n",
    "\n",
    "def load_file(src_path: str, dst_path):\n",
    "    '''Get the first file in the source directory, and move it to the destination directory'''\n",
    "    try:\n",
    "        '''First, ensure the destination directory exists.'''\n",
    "        create_directory(dst_path)\n",
    "        \n",
    "        file = sorted(os.listdir(src_path))[0]\n",
    "        src_file_path = os.path.join(src_path, file)\n",
    "        dst_file_path = os.path.join(dst_path, file)\n",
    "        \n",
    "        retval = shutil.move(src_file_path, dst_file_path)\n",
    "\n",
    "        dst_dir = os.path.split(dst_path)[1]\n",
    "        if retval:\n",
    "            return f\"The file '{file}' has been successfully moved to the directory '{dst_dir}'.\"\n",
    "        else:\n",
    "            return f\"The file '{file}' was not moved to the directory '{dst_dir}'.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occured: {e}\"\n",
    "\n",
    "\n",
    "def wait_until_stream_is_ready(query, min_batches=1):\n",
    "    while len(query.recentProgress) < min_batches:\n",
    "        time.sleep(5)\n",
    "        \n",
    "    print(f\"The stream has processed {len(query.recentProgress)} batchs\")\n",
    "\n",
    "\n",
    "def restore_files(src_path: str, dst_path: str):\n",
    "    '''Move all files in the source directory to the destination directory'''\n",
    "    try:\n",
    "        for file in os.listdir(src_path):\n",
    "            src_file_path = os.path.join(src_path, file)\n",
    "            dst_file_path = os.path.join(dst_path, file)\n",
    "            retval = shutil.move(src_file_path, dst_file_path)\n",
    "            dst_dir = os.path.split(dst_path)[1]\n",
    "            if retval:\n",
    "                print(f\"The file '{file}' has been successfully moved to the directory '{dst_dir}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occured: {e}\"\n",
    "\n",
    "\n",
    "def remove_directory_tree(path: str):\n",
    "    '''If it exists, remove the entire contents of a directory structure at a given 'path' parameter's location.'''\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' has been removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dacc437-ab4f-4ef8-8757-908ee3ff2ca1",
   "metadata": {},
   "source": [
    "#### 1.4. Prepare the file system environment\n",
    "- Remove the Data Lakehouse Database Directory Structure to Ensure Idempotency\n",
    "- List all files in the specified directory to ensure their existance and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c11dacb-0678-4cbe-a3a4-412558915093",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory_tree(database_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e02682d-c279-4bec-a63a-3ba76be33bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file_info(tracker_source_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def01700-22b8-4f9d-822d-10defe297b67",
   "metadata": {},
   "source": [
    "#### 1.5. Create a New Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730ec835-067a-4dc7-882a-2a0ab6e387f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "shuffle_partitions = int(os.cpu_count())\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder \\\n",
    "    .appName('PySpark Customers Delta Table in Juptyer')\\\n",
    "    .master(worker_threads)\\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .config('spark.executor.memory', '2g')\\\n",
    "    .config('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog') \\\n",
    "    .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension') \\\n",
    "    .config('spark.sql.shuffle.partitions', shuffle_partitions) \\\n",
    "    .config('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .config('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .config('spark.sql.warehouse.dir', database_dir) \\\n",
    "    .config('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31287089-581a-4ad8-a1fb-09f4d29e628e",
   "metadata": {},
   "source": [
    "### 2.0. Using Spark Structured Streaming for Incremental Ingestion\n",
    "In the cell below, a function is defined to demonstrate using Apache Spark Structured Streaming with the PySpark API. This code includes both a Structured Streaming read and write. If you wish to learn more about options for creating streaming dataframes and datasets, refer to the <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#creating-streaming-dataframes-and-streaming-datasets\" target=\"_blank\">documentation</a>.\n",
    "\n",
    "Note that when using Structured Streaming with automatic <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#schema-inference-and-partition-of-streaming-dataframesdatasets\" target=\"_blank\">schema inference and evolution</a>, the 4 arguments shown here should allow ingestion of most datasets. These arguments are explained below.\n",
    "\n",
    "| argument | what it is | how it's used |\n",
    "| --- | --- | --- |\n",
    "| **`data_source`** | The directory of the source data | Spark will detect new files as they arrive in this location and queue them for ingestion; passed to the **`.load()`** method |\n",
    "| **`source_format`** | The format of the source data |  The format of the source data should always be specified for the **`format`** option |\n",
    "| **`table_name`** | The name of the target table | Spark Structured Streaming supports writing directly to Delta Lake tables by passing a table name as a string to the **`.table()`** method. Note that you can either append to an existing table or create a new table |\n",
    "| **`checkpoint_directory`** | The location for storing metadata about the stream | This argument is passed to the **`checkpointLocation`** and **`schemaLocation`** options. Checkpoints keep track of streaming progress, while the schema location tracks updates to the fields in the source dataset |\n",
    "\n",
    "**NOTE**: The code below has been streamlined to demonstrate Structured Streaming functionality. Later we'll see how additional transformations can be applied to source data before saving them to Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d803e-a886-48d2-b55d-94a01caafc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_table(data_source, source_format, table_name, checkpoint_directory):\n",
    "    query = (spark.readStream \\\n",
    "             .format(source_format) \\\n",
    "             .option(\"schemaLocation\", checkpoint_directory) \\\n",
    "             .option(\"maxFilesPerTrigger\", 1) \\\n",
    "             .option(\"multiLine\", \"true\") \\\n",
    "             .option(\"inferSchema\", \"true\") \\\n",
    "             .load(data_source) \\\n",
    "             .writeStream \\\n",
    "             .format(\"delta\") \\\n",
    "             .outputMode(\"append\") \\\n",
    "             .option(\"checkpointLocation\", checkpoint_directory) \\\n",
    "             .option(\"mergeSchema\", \"true\") \\\n",
    "             .toTable(table_name))\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf57a642-4bd1-4895-99cc-131cabc76759",
   "metadata": {},
   "source": [
    "We will now use the previously defined function and some path variables defined in the setup script to begin a structured streaming process that reads from a source directory of JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00f7a4-4415-4033-a3e0-541d49af2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file(tracker_source_dir, tracker_files_processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6c0e80-2430-4c1c-85d8-6cc1b076a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_checkpoint = os.path.join(tracker_output, '_checkpoint')\n",
    "\n",
    "query = load_to_table(data_source = tracker_files_processed_dir, \\\n",
    "                      source_format = \"json\", \\\n",
    "                      table_name = \"heartbeat_tracker\", \\\n",
    "                      checkpoint_directory = tracker_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d78a6c3-519b-4b57-a744-a00772cc32cf",
   "metadata": {},
   "source": [
    "Because Spark Structured Streaming loads data incrementally, the code above doesn't appear to finish executing. That's because this is a **continuously active query**, which means that as soon as new data arrives in our data source, it will be processed through our logic and loaded into our target table.\n",
    "\n",
    "#### 2.1. Helper Function for Streaming Lessons\n",
    "This lesson demonstrates combining streaming functions with batch and streaming queries against the results of their operations. This notebook is for instructional purposes and intended for interactive, cell-by-cell execution. That being said, here we define a helper function to prevent our notebook from executing the next cell just long enough to ensure data has been written out by a given streaming query. This code would not be necessary in a production job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229c201-15a6-4ef0-b7e0-ca98df1a4a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_until_stream_is_ready(query, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bf5ea-4d47-424f-92a8-ec0a839c1b10",
   "metadata": {},
   "source": [
    "### 3.0. Query Target Table\n",
    "Once data has been ingested into Delta Lake, it can be interacted with in just the same manner as any SQL database table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c54900-d925-46db-b53f-c1d41db14865",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM heartbeat_tracker\").toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5161d61-b1a8-4cef-98f2-3f7a1542a6b5",
   "metadata": {},
   "source": [
    "#### 3.1. Inspect the Streaming Table\n",
    "While the field names for our data have been captured correctly, note that it encoded all fields as **`STRING`** type. Because JSON is a text-based format, this is the safest and most permissive type, ensuring that the least amount of data is dropped or ignored at ingestion due to type mismatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb15e2e3-d1a6-4618-b0ae-074163a4da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE TABLE heartbeat_tracker\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b90a7d-eac1-4b54-9ad4-4ed96ab791ff",
   "metadata": {},
   "source": [
    "#### 3.2. Create a Temporary View\n",
    "Here we'll define a temporary view that summarizes the recordings in our target table.  This view will be used to demonstrate how new data is automatically ingested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7268562a-b576-41dc-8748-15dc3f0f78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_tempview = \"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW device_counts AS\n",
    "  SELECT device_id, COUNT(*) AS total_recordings\n",
    "  FROM heartbeat_tracker\n",
    "  GROUP BY device_id;\n",
    "\"\"\"\n",
    "spark.sql(sql_tempview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c736be4-f8e0-45f4-b625-745dc733cee7",
   "metadata": {},
   "source": [
    "#### 3.2. Query the Temporary View\n",
    "Now we can query the temporary view to inspect the progress of incrememtal data ingestion by our structured streaming query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f91a06-facd-41a1-87c8-5284ede56459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_device_counts(records: int):\n",
    "    sql_device_counts = f\"\"\"\n",
    "        SELECT device_id AS `Device ID`\n",
    "            , total_recordings AS `Total Recordings`\n",
    "        FROM device_counts\n",
    "        ORDER BY total_recordings DESC\n",
    "        LIMIT {records}\n",
    "    \"\"\"\n",
    "    dframe = spark.sql(sql_device_counts).toPandas()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "display_device_counts(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c4582a-3645-42fb-b0e6-cffcb2a07703",
   "metadata": {},
   "source": [
    "### 4.0. Land New Data\n",
    "\n",
    "As mentioned previously, Spark Structured Streaming is configured to incrementally process files from a directory in object storage into a Delta Lake table.  We have configured and are currently executing a query to process JSON files from the location specified by **`source_path`** into a table named **`heartbeat_tracker`**.\n",
    "\n",
    "#### 4.1. Inspect Data Source\n",
    "First, let's review the contents of the **`source_path`** directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3291d596-4dd1-439b-a1bd-dc76da197f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file_info(tracker_files_processed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cddadb-b478-473a-9430-b37059ac5b2d",
   "metadata": {},
   "source": [
    "#### 4.2. Load Another JSON Data File\n",
    "At present, you should see a single JSON file listed in this location. The function invoked in the following cell was configured by our setup script to allow us to mimic an external system writing new data to this directory. Each time you execute the cell below, a new file will land in the **`source_path`** directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0d5054-e749-44d9-b787-595de3d2d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file(tracker_source_dir, tracker_files_processed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99297304-53da-456e-b60e-9bdc793eccf9",
   "metadata": {},
   "source": [
    "#### 4.3. Confirm the New JSON File\n",
    "When we list the contents of the **`source_path`** again using the cell below we should see an additional JSON file for each time you ran the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a108a-a6e2-4bcc-aa5f-63605181b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file_info(tracker_files_processed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1140bcfb-2d09-4b39-be17-41686ddecc52",
   "metadata": {},
   "source": [
    "### 5.0. Tracking Ingestion Progress\n",
    "Historically, many systems have been configured to either reprocess all records in a source directory to calculate current results or require data engineers to implement custom logic to identify new data that's arrived since the last time a table was updated.  With Spark Structured Streaming, your table has already been updated. Run the query below to confirm that new data has been ingested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a5726-7702-4c24-9ca6-9bec6e51278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_until_stream_is_ready(query, 1)\n",
    "\n",
    "display_device_counts(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813be24e-c4ad-4dc1-87a8-91a002ea30a2",
   "metadata": {},
   "source": [
    "The Spark streaming query we configured earlier automatically detects and processes records from the source directory into the target table. While there is a slight delay as records are ingested, a streaming query executing with default streaming configuration should update results in near real time.  The query below shows the table history. A new table version should be indicated for each **`STREAMING UPDATE`**. These update events coincide with new batches of data arriving at the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9bd14-fe13-480e-87d2-57fa95a61b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE HISTORY heartbeat_tracker\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e0e4aa-bb4a-4015-8c68-6bc2c9a66965",
   "metadata": {},
   "source": [
    "### 6.0. Clean Up\n",
    "- Restore JSON files to their Original Storage Location\n",
    "- Stop the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf05c55-9f1c-4d39-ae9d-5e19d6d226b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_files(tracker_files_processed_dir, tracker_source_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e999c716-9939-4b47-8ee7-18fbe91573b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (3.12-env)",
   "language": "python",
   "name": "3.12-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

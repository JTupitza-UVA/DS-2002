{"cells":[{"cell_type":"markdown","source":["### Getting Started with Databricks Data Engineering\nThere are three key Apache Spark interfaces that you should know about: Resilient Distributed Dataset, DataFrame, and Dataset.\n\n- Resilient Distributed Dataset: The first Apache Spark abstraction was the Resilient Distributed Dataset (RDD). It is an interface to a sequence of data objects that consist of one or more types that are located across a collection of machines (a cluster). RDDs can be created in a variety of ways and are the “lowest level” API available. While this is the original data structure for Apache Spark, you should focus on the DataFrame API, which is a superset of the RDD functionality. The RDD API is available in the Java, Python, and Scala languages.\n- DataFrame: These are similar in concept to the DataFrame you may be familiar with in the pandas Python library and the R language. The DataFrame API is available in the Java, Python, R, and Scala languages.\n- Dataset: A combination of DataFrame and RDD. It provides the typed interface that is available in RDDs while providing the convenience of the DataFrame. The Dataset API is available in the Java and Scala languages.\n\nIn many scenarios, especially with the performance optimizations embedded in DataFrames and Datasets, it will not be necessary to work with RDDs. But it is important to understand the RDD abstraction because:\n- The RDD is the underlying infrastructure that allows Spark to run so fast and provide data lineage.\n- If you are diving into more advanced components of Spark, it may be necessary to use RDDs.\n- The visualizations within the Spark UI reference RDDs.\n\n#### 1.0. Apache Spark File Utilities\nDatabricks Utilities `(dbutils)` make it easy to perform powerful combinations of tasks. You can use the utilities to work with object storage efficiently, to chain and parameterize notebooks, and to work with secrets. `dbutils` are not supported outside of notebooks.\n\n##### 1.1. List available commands for a utility\nTo list available commands for a utility along with a short description of each command, run `.help()` after the programmatic name for the utility. For example, the following command lists the available commands for the Databricks File System (DBFS) utility."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b8c8c54-01ec-4134-8071-e0d9695cfd8c"}}},{"cell_type":"code","source":["dbutils.fs.help()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6b4618c-b901-4da3-8645-f1eeee5e1bc3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class = \"ansiout\"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in\nthis package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or\nanother FileSystem URI.\n\nFor more info about a method, use <b>dbutils.fs.help(\"methodName\")</b>.\n\nIn notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps\nstraightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\"\ntranslates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\".\n    <h3>fsutils</h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -> Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -> Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -> Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -> Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -> Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -> Removes a file or directory<br /><br /><h3>mount</h3><b>mount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -> Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -> Deletes a DBFS mount point<br /><br /></div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div class = \"ansiout\"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in\nthis package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or\nanother FileSystem URI.\n\nFor more info about a method, use <b>dbutils.fs.help(\"methodName\")</b>.\n\nIn notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps\nstraightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\"\ntranslates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\".\n    <h3>fsutils</h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -> Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -> Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -> Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -> Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -> Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -> Removes a file or directory<br /><br /><h3>mount</h3><b>mount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -> Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -> Deletes a DBFS mount point<br /><br /></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### 1.2. List the Contents of a Folder\nThe following command lists the contents of a folder in the Databricks File System (DBFS). In this example, all the sample datasets are being enumerated."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28d68e47-d804-4470-b340-57c647931751"}}},{"cell_type":"code","source":["display(dbutils.fs.ls('/databricks-datasets'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43cc472a-7483-4d26-bb44-4674a1af5e34"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/databricks-datasets/COVID/","COVID/",0],["dbfs:/databricks-datasets/README.md","README.md",976],["dbfs:/databricks-datasets/Rdatasets/","Rdatasets/",0],["dbfs:/databricks-datasets/SPARK_README.md","SPARK_README.md",3359],["dbfs:/databricks-datasets/adult/","adult/",0],["dbfs:/databricks-datasets/airlines/","airlines/",0],["dbfs:/databricks-datasets/amazon/","amazon/",0],["dbfs:/databricks-datasets/asa/","asa/",0],["dbfs:/databricks-datasets/atlas_higgs/","atlas_higgs/",0],["dbfs:/databricks-datasets/bikeSharing/","bikeSharing/",0],["dbfs:/databricks-datasets/cctvVideos/","cctvVideos/",0],["dbfs:/databricks-datasets/credit-card-fraud/","credit-card-fraud/",0],["dbfs:/databricks-datasets/cs100/","cs100/",0],["dbfs:/databricks-datasets/cs110x/","cs110x/",0],["dbfs:/databricks-datasets/cs190/","cs190/",0],["dbfs:/databricks-datasets/data.gov/","data.gov/",0],["dbfs:/databricks-datasets/definitive-guide/","definitive-guide/",0],["dbfs:/databricks-datasets/delta-sharing/","delta-sharing/",0],["dbfs:/databricks-datasets/flights/","flights/",0],["dbfs:/databricks-datasets/flower_photos/","flower_photos/",0],["dbfs:/databricks-datasets/flowers/","flowers/",0],["dbfs:/databricks-datasets/genomics/","genomics/",0],["dbfs:/databricks-datasets/hail/","hail/",0],["dbfs:/databricks-datasets/iot/","iot/",0],["dbfs:/databricks-datasets/iot-stream/","iot-stream/",0],["dbfs:/databricks-datasets/learning-spark/","learning-spark/",0],["dbfs:/databricks-datasets/learning-spark-v2/","learning-spark-v2/",0],["dbfs:/databricks-datasets/lending-club-loan-stats/","lending-club-loan-stats/",0],["dbfs:/databricks-datasets/med-images/","med-images/",0],["dbfs:/databricks-datasets/mnist-digits/","mnist-digits/",0],["dbfs:/databricks-datasets/news20.binary/","news20.binary/",0],["dbfs:/databricks-datasets/nyctaxi/","nyctaxi/",0],["dbfs:/databricks-datasets/nyctaxi-with-zipcodes/","nyctaxi-with-zipcodes/",0],["dbfs:/databricks-datasets/online_retail/","online_retail/",0],["dbfs:/databricks-datasets/overlap-join/","overlap-join/",0],["dbfs:/databricks-datasets/power-plant/","power-plant/",0],["dbfs:/databricks-datasets/retail-org/","retail-org/",0],["dbfs:/databricks-datasets/rwe/","rwe/",0],["dbfs:/databricks-datasets/sai-summit-2019-sf/","sai-summit-2019-sf/",0],["dbfs:/databricks-datasets/sample_logs/","sample_logs/",0],["dbfs:/databricks-datasets/samples/","samples/",0],["dbfs:/databricks-datasets/sfo_customer_survey/","sfo_customer_survey/",0],["dbfs:/databricks-datasets/sms_spam_collection/","sms_spam_collection/",0],["dbfs:/databricks-datasets/songs/","songs/",0],["dbfs:/databricks-datasets/structured-streaming/","structured-streaming/",0],["dbfs:/databricks-datasets/timeseries/","timeseries/",0],["dbfs:/databricks-datasets/tpch/","tpch/",0],["dbfs:/databricks-datasets/warmup/","warmup/",0],["dbfs:/databricks-datasets/weather/","weather/",0],["dbfs:/databricks-datasets/wiki/","wiki/",0],["dbfs:/databricks-datasets/wikipedia-datasets/","wikipedia-datasets/",0],["dbfs:/databricks-datasets/wine-quality/","wine-quality/",0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/COVID/</td><td>COVID/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/README.md</td><td>README.md</td><td>976</td></tr><tr><td>dbfs:/databricks-datasets/Rdatasets/</td><td>Rdatasets/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/SPARK_README.md</td><td>SPARK_README.md</td><td>3359</td></tr><tr><td>dbfs:/databricks-datasets/adult/</td><td>adult/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/airlines/</td><td>airlines/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/amazon/</td><td>amazon/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/asa/</td><td>asa/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/atlas_higgs/</td><td>atlas_higgs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/bikeSharing/</td><td>bikeSharing/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cctvVideos/</td><td>cctvVideos/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/credit-card-fraud/</td><td>credit-card-fraud/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs100/</td><td>cs100/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/</td><td>cs110x/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs190/</td><td>cs190/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/data.gov/</td><td>data.gov/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/definitive-guide/</td><td>definitive-guide/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/delta-sharing/</td><td>delta-sharing/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flights/</td><td>flights/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flower_photos/</td><td>flower_photos/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flowers/</td><td>flowers/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/genomics/</td><td>genomics/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/hail/</td><td>hail/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot/</td><td>iot/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot-stream/</td><td>iot-stream/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark/</td><td>learning-spark/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark-v2/</td><td>learning-spark-v2/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/lending-club-loan-stats/</td><td>lending-club-loan-stats/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/med-images/</td><td>med-images/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/mnist-digits/</td><td>mnist-digits/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/news20.binary/</td><td>news20.binary/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi/</td><td>nyctaxi/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi-with-zipcodes/</td><td>nyctaxi-with-zipcodes/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/online_retail/</td><td>online_retail/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/overlap-join/</td><td>overlap-join/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/</td><td>power-plant/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/retail-org/</td><td>retail-org/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/rwe/</td><td>rwe/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sai-summit-2019-sf/</td><td>sai-summit-2019-sf/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sample_logs/</td><td>sample_logs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/samples/</td><td>samples/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sfo_customer_survey/</td><td>sfo_customer_survey/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sms_spam_collection/</td><td>sms_spam_collection/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/songs/</td><td>songs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/</td><td>structured-streaming/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/timeseries/</td><td>timeseries/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/tpch/</td><td>tpch/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/warmup/</td><td>warmup/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/weather/</td><td>weather/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wiki/</td><td>wiki/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wikipedia-datasets/</td><td>wikipedia-datasets/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wine-quality/</td><td>wine-quality/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### 1.3. Create a Table based on a Databricks Dataset\nThis code example demonstrates how to use SQL in the Databricks SQL query editor, or how to use Python in a notebook in Data Science & Engineering or Databricks Machine Learning, to create a table based on a Databricks dataset:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65396bff-7956-4a7f-a4af-ef69207ff9f9"}}},{"cell_type":"code","source":["%sql\nDROP TABLE default.people10m;\n\nCREATE TABLE default.people10m\n  OPTIONS (PATH 'dbfs:/databricks-datasets/learning-spark-v2/people/people-10m.delta');\n  \nSELECT * FROM default.people10m LIMIT 10;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba47379e-4685-4fcc-8206-ebfb7a562808"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[3766824,"Hisako","Isabella","Malitrott","F","1961-02-12T05:00:00.000+0000","938-80-1874",58862],[3766825,"Daisy","Merissa","Fibben","F","1998-05-19T04:00:00.000+0000","971-14-3755",66221],[3766826,"Caren","Blossom","Henner","F","1962-08-06T04:00:00.000+0000","954-19-8973",54376],[3766827,"Darleen","Gertie","Goodinson","F","1980-03-12T05:00:00.000+0000","981-65-5269",69954],[3766828,"Kyle","Lu","Habben","F","1974-02-15T04:00:00.000+0000","936-95-3240",56681],[3766829,"Melia","Kristy","Bonhill","F","1970-09-13T04:00:00.000+0000","960-91-9232",73995],[3766830,"Yevette","Faye","Bebbell","F","1972-09-07T04:00:00.000+0000","987-72-3701",92888],[3766831,"Delpha","Kenisha","Gillison","F","1979-06-25T04:00:00.000+0000","962-66-5404",51206],[3766832,"Mikaela","Jenifer","Hallan","F","1973-05-23T04:00:00.000+0000","911-38-3114",98887],[3766833,"Cindi","Renita","Cousin","F","1979-03-19T05:00:00.000+0000","666-50-3216",63646]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"integer\"","metadata":"{}"},{"name":"firstName","type":"\"string\"","metadata":"{}"},{"name":"middleName","type":"\"string\"","metadata":"{}"},{"name":"lastName","type":"\"string\"","metadata":"{}"},{"name":"gender","type":"\"string\"","metadata":"{}"},{"name":"birthDate","type":"\"timestamp\"","metadata":"{}"},{"name":"ssn","type":"\"string\"","metadata":"{}"},{"name":"salary","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>firstName</th><th>middleName</th><th>lastName</th><th>gender</th><th>birthDate</th><th>ssn</th><th>salary</th></tr></thead><tbody><tr><td>3766824</td><td>Hisako</td><td>Isabella</td><td>Malitrott</td><td>F</td><td>1961-02-12T05:00:00.000+0000</td><td>938-80-1874</td><td>58862</td></tr><tr><td>3766825</td><td>Daisy</td><td>Merissa</td><td>Fibben</td><td>F</td><td>1998-05-19T04:00:00.000+0000</td><td>971-14-3755</td><td>66221</td></tr><tr><td>3766826</td><td>Caren</td><td>Blossom</td><td>Henner</td><td>F</td><td>1962-08-06T04:00:00.000+0000</td><td>954-19-8973</td><td>54376</td></tr><tr><td>3766827</td><td>Darleen</td><td>Gertie</td><td>Goodinson</td><td>F</td><td>1980-03-12T05:00:00.000+0000</td><td>981-65-5269</td><td>69954</td></tr><tr><td>3766828</td><td>Kyle</td><td>Lu</td><td>Habben</td><td>F</td><td>1974-02-15T04:00:00.000+0000</td><td>936-95-3240</td><td>56681</td></tr><tr><td>3766829</td><td>Melia</td><td>Kristy</td><td>Bonhill</td><td>F</td><td>1970-09-13T04:00:00.000+0000</td><td>960-91-9232</td><td>73995</td></tr><tr><td>3766830</td><td>Yevette</td><td>Faye</td><td>Bebbell</td><td>F</td><td>1972-09-07T04:00:00.000+0000</td><td>987-72-3701</td><td>92888</td></tr><tr><td>3766831</td><td>Delpha</td><td>Kenisha</td><td>Gillison</td><td>F</td><td>1979-06-25T04:00:00.000+0000</td><td>962-66-5404</td><td>51206</td></tr><tr><td>3766832</td><td>Mikaela</td><td>Jenifer</td><td>Hallan</td><td>F</td><td>1973-05-23T04:00:00.000+0000</td><td>911-38-3114</td><td>98887</td></tr><tr><td>3766833</td><td>Cindi</td><td>Renita</td><td>Cousin</td><td>F</td><td>1979-03-19T05:00:00.000+0000</td><td>666-50-3216</td><td>63646</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"DROP TABLE default.people10m2\")\nspark.sql(\"CREATE TABLE default.people10m2 OPTIONS (PATH 'dbfs:/databricks-datasets/learning-spark-v2/people/people-10m.delta')\")\n\ndf = spark.sql(\"SELECT * FROM default.people10m2 LIMIT 10\")\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f72859e6-ba53-49f1-aa52-76d13470252e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[3766824,"Hisako","Isabella","Malitrott","F","1961-02-12T05:00:00.000+0000","938-80-1874",58862],[3766825,"Daisy","Merissa","Fibben","F","1998-05-19T04:00:00.000+0000","971-14-3755",66221],[3766826,"Caren","Blossom","Henner","F","1962-08-06T04:00:00.000+0000","954-19-8973",54376],[3766827,"Darleen","Gertie","Goodinson","F","1980-03-12T05:00:00.000+0000","981-65-5269",69954],[3766828,"Kyle","Lu","Habben","F","1974-02-15T04:00:00.000+0000","936-95-3240",56681],[3766829,"Melia","Kristy","Bonhill","F","1970-09-13T04:00:00.000+0000","960-91-9232",73995],[3766830,"Yevette","Faye","Bebbell","F","1972-09-07T04:00:00.000+0000","987-72-3701",92888],[3766831,"Delpha","Kenisha","Gillison","F","1979-06-25T04:00:00.000+0000","962-66-5404",51206],[3766832,"Mikaela","Jenifer","Hallan","F","1973-05-23T04:00:00.000+0000","911-38-3114",98887],[3766833,"Cindi","Renita","Cousin","F","1979-03-19T05:00:00.000+0000","666-50-3216",63646]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"integer\"","metadata":"{}"},{"name":"firstName","type":"\"string\"","metadata":"{}"},{"name":"middleName","type":"\"string\"","metadata":"{}"},{"name":"lastName","type":"\"string\"","metadata":"{}"},{"name":"gender","type":"\"string\"","metadata":"{}"},{"name":"birthDate","type":"\"timestamp\"","metadata":"{}"},{"name":"ssn","type":"\"string\"","metadata":"{}"},{"name":"salary","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>firstName</th><th>middleName</th><th>lastName</th><th>gender</th><th>birthDate</th><th>ssn</th><th>salary</th></tr></thead><tbody><tr><td>3766824</td><td>Hisako</td><td>Isabella</td><td>Malitrott</td><td>F</td><td>1961-02-12T05:00:00.000+0000</td><td>938-80-1874</td><td>58862</td></tr><tr><td>3766825</td><td>Daisy</td><td>Merissa</td><td>Fibben</td><td>F</td><td>1998-05-19T04:00:00.000+0000</td><td>971-14-3755</td><td>66221</td></tr><tr><td>3766826</td><td>Caren</td><td>Blossom</td><td>Henner</td><td>F</td><td>1962-08-06T04:00:00.000+0000</td><td>954-19-8973</td><td>54376</td></tr><tr><td>3766827</td><td>Darleen</td><td>Gertie</td><td>Goodinson</td><td>F</td><td>1980-03-12T05:00:00.000+0000</td><td>981-65-5269</td><td>69954</td></tr><tr><td>3766828</td><td>Kyle</td><td>Lu</td><td>Habben</td><td>F</td><td>1974-02-15T04:00:00.000+0000</td><td>936-95-3240</td><td>56681</td></tr><tr><td>3766829</td><td>Melia</td><td>Kristy</td><td>Bonhill</td><td>F</td><td>1970-09-13T04:00:00.000+0000</td><td>960-91-9232</td><td>73995</td></tr><tr><td>3766830</td><td>Yevette</td><td>Faye</td><td>Bebbell</td><td>F</td><td>1972-09-07T04:00:00.000+0000</td><td>987-72-3701</td><td>92888</td></tr><tr><td>3766831</td><td>Delpha</td><td>Kenisha</td><td>Gillison</td><td>F</td><td>1979-06-25T04:00:00.000+0000</td><td>962-66-5404</td><td>51206</td></tr><tr><td>3766832</td><td>Mikaela</td><td>Jenifer</td><td>Hallan</td><td>F</td><td>1973-05-23T04:00:00.000+0000</td><td>911-38-3114</td><td>98887</td></tr><tr><td>3766833</td><td>Cindi</td><td>Renita</td><td>Cousin</td><td>F</td><td>1979-03-19T05:00:00.000+0000</td><td>666-50-3216</td><td>63646</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 2.0. Structured Streaming\nSensors, IoT devices, social networks, and online transactions all generate data that needs to be monitored constantly and acted upon quickly. As a result, the need for large-scale, real-time stream processing is more evident than ever before. This tutorial module introduces Structured Streaming, the main model for handling streaming datasets in Apache Spark. In Structured Streaming, a data stream is treated as a table that is being continuously appended. This leads to a stream processing model that is very similar to a batch processing model. You express your streaming computation as a standard batch-like query as on a static table, but Spark runs it as an incremental query on the unbounded input table.\n\n#### 2.1. Load the Sample Data\nThe easiest way to get started with Structured Streaming is to use an example Azure Databricks dataset available in the /databricks-datasets folder accessible within the Azure Databricks workspace. Azure Databricks has sample event data as files in /databricks-datasets/structured-streaming/events/ to use to build a Structured Streaming application. First, take a look at the contents of this directory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16c99dcc-88c8-478b-8d57-a50fd5ed6739"}}},{"cell_type":"code","source":["inputPath = \"/databricks-datasets/structured-streaming/events/\"\ndisplay(dbutils.fs.ls(inputPath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6501049c-f061-4a97-a270-ac2613229b5c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/databricks-datasets/structured-streaming/events/file-0.json","file-0.json",72530],["dbfs:/databricks-datasets/structured-streaming/events/file-1.json","file-1.json",72961],["dbfs:/databricks-datasets/structured-streaming/events/file-10.json","file-10.json",73025],["dbfs:/databricks-datasets/structured-streaming/events/file-11.json","file-11.json",72999],["dbfs:/databricks-datasets/structured-streaming/events/file-12.json","file-12.json",72987],["dbfs:/databricks-datasets/structured-streaming/events/file-13.json","file-13.json",73006],["dbfs:/databricks-datasets/structured-streaming/events/file-14.json","file-14.json",73003],["dbfs:/databricks-datasets/structured-streaming/events/file-15.json","file-15.json",73007],["dbfs:/databricks-datasets/structured-streaming/events/file-16.json","file-16.json",72978],["dbfs:/databricks-datasets/structured-streaming/events/file-17.json","file-17.json",73008],["dbfs:/databricks-datasets/structured-streaming/events/file-18.json","file-18.json",73002],["dbfs:/databricks-datasets/structured-streaming/events/file-19.json","file-19.json",73014],["dbfs:/databricks-datasets/structured-streaming/events/file-2.json","file-2.json",73007],["dbfs:/databricks-datasets/structured-streaming/events/file-20.json","file-20.json",72987],["dbfs:/databricks-datasets/structured-streaming/events/file-21.json","file-21.json",72983],["dbfs:/databricks-datasets/structured-streaming/events/file-22.json","file-22.json",73009],["dbfs:/databricks-datasets/structured-streaming/events/file-23.json","file-23.json",72985],["dbfs:/databricks-datasets/structured-streaming/events/file-24.json","file-24.json",73020],["dbfs:/databricks-datasets/structured-streaming/events/file-25.json","file-25.json",72980],["dbfs:/databricks-datasets/structured-streaming/events/file-26.json","file-26.json",73002],["dbfs:/databricks-datasets/structured-streaming/events/file-27.json","file-27.json",73013],["dbfs:/databricks-datasets/structured-streaming/events/file-28.json","file-28.json",73005],["dbfs:/databricks-datasets/structured-streaming/events/file-29.json","file-29.json",72977],["dbfs:/databricks-datasets/structured-streaming/events/file-3.json","file-3.json",72996],["dbfs:/databricks-datasets/structured-streaming/events/file-30.json","file-30.json",73009],["dbfs:/databricks-datasets/structured-streaming/events/file-31.json","file-31.json",73008],["dbfs:/databricks-datasets/structured-streaming/events/file-32.json","file-32.json",72982],["dbfs:/databricks-datasets/structured-streaming/events/file-33.json","file-33.json",73033],["dbfs:/databricks-datasets/structured-streaming/events/file-34.json","file-34.json",72985],["dbfs:/databricks-datasets/structured-streaming/events/file-35.json","file-35.json",72974],["dbfs:/databricks-datasets/structured-streaming/events/file-36.json","file-36.json",73013],["dbfs:/databricks-datasets/structured-streaming/events/file-37.json","file-37.json",72989],["dbfs:/databricks-datasets/structured-streaming/events/file-38.json","file-38.json",72999],["dbfs:/databricks-datasets/structured-streaming/events/file-39.json","file-39.json",73013],["dbfs:/databricks-datasets/structured-streaming/events/file-4.json","file-4.json",72992],["dbfs:/databricks-datasets/structured-streaming/events/file-40.json","file-40.json",72986],["dbfs:/databricks-datasets/structured-streaming/events/file-41.json","file-41.json",73019],["dbfs:/databricks-datasets/structured-streaming/events/file-42.json","file-42.json",72986],["dbfs:/databricks-datasets/structured-streaming/events/file-43.json","file-43.json",72990],["dbfs:/databricks-datasets/structured-streaming/events/file-44.json","file-44.json",73018],["dbfs:/databricks-datasets/structured-streaming/events/file-45.json","file-45.json",72997],["dbfs:/databricks-datasets/structured-streaming/events/file-46.json","file-46.json",72991],["dbfs:/databricks-datasets/structured-streaming/events/file-47.json","file-47.json",73009],["dbfs:/databricks-datasets/structured-streaming/events/file-48.json","file-48.json",72993],["dbfs:/databricks-datasets/structured-streaming/events/file-49.json","file-49.json",73496],["dbfs:/databricks-datasets/structured-streaming/events/file-5.json","file-5.json",72998],["dbfs:/databricks-datasets/structured-streaming/events/file-6.json","file-6.json",72997],["dbfs:/databricks-datasets/structured-streaming/events/file-7.json","file-7.json",73022],["dbfs:/databricks-datasets/structured-streaming/events/file-8.json","file-8.json",72997],["dbfs:/databricks-datasets/structured-streaming/events/file-9.json","file-9.json",72970]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-0.json</td><td>file-0.json</td><td>72530</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-1.json</td><td>file-1.json</td><td>72961</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-10.json</td><td>file-10.json</td><td>73025</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-11.json</td><td>file-11.json</td><td>72999</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-12.json</td><td>file-12.json</td><td>72987</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-13.json</td><td>file-13.json</td><td>73006</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-14.json</td><td>file-14.json</td><td>73003</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-15.json</td><td>file-15.json</td><td>73007</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-16.json</td><td>file-16.json</td><td>72978</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-17.json</td><td>file-17.json</td><td>73008</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-18.json</td><td>file-18.json</td><td>73002</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-19.json</td><td>file-19.json</td><td>73014</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-2.json</td><td>file-2.json</td><td>73007</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-20.json</td><td>file-20.json</td><td>72987</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-21.json</td><td>file-21.json</td><td>72983</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-22.json</td><td>file-22.json</td><td>73009</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-23.json</td><td>file-23.json</td><td>72985</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-24.json</td><td>file-24.json</td><td>73020</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-25.json</td><td>file-25.json</td><td>72980</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-26.json</td><td>file-26.json</td><td>73002</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-27.json</td><td>file-27.json</td><td>73013</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-28.json</td><td>file-28.json</td><td>73005</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-29.json</td><td>file-29.json</td><td>72977</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-3.json</td><td>file-3.json</td><td>72996</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-30.json</td><td>file-30.json</td><td>73009</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-31.json</td><td>file-31.json</td><td>73008</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-32.json</td><td>file-32.json</td><td>72982</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-33.json</td><td>file-33.json</td><td>73033</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-34.json</td><td>file-34.json</td><td>72985</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-35.json</td><td>file-35.json</td><td>72974</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-36.json</td><td>file-36.json</td><td>73013</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-37.json</td><td>file-37.json</td><td>72989</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-38.json</td><td>file-38.json</td><td>72999</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-39.json</td><td>file-39.json</td><td>73013</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-4.json</td><td>file-4.json</td><td>72992</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-40.json</td><td>file-40.json</td><td>72986</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-41.json</td><td>file-41.json</td><td>73019</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-42.json</td><td>file-42.json</td><td>72986</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-43.json</td><td>file-43.json</td><td>72990</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-44.json</td><td>file-44.json</td><td>73018</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-45.json</td><td>file-45.json</td><td>72997</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-46.json</td><td>file-46.json</td><td>72991</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-47.json</td><td>file-47.json</td><td>73009</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-48.json</td><td>file-48.json</td><td>72993</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-49.json</td><td>file-49.json</td><td>73496</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-5.json</td><td>file-5.json</td><td>72998</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-6.json</td><td>file-6.json</td><td>72997</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-7.json</td><td>file-7.json</td><td>73022</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-8.json</td><td>file-8.json</td><td>72997</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/events/file-9.json</td><td>file-9.json</td><td>72970</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 2.2. Initialize the Stream\nSince the sample data is just a static set of files, you can emulate a stream from them by reading one file at a time, in the chronological order in which they were created."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d825249d-7165-4948-a912-bebdbda6be9e"}}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\n# Define the schema to speed up processing\njsonSchema = StructType([ StructField(\"time\", TimestampType(), True), StructField(\"action\", StringType(), True) ])\n\nstreamingInputDF = (\n  spark\n    .readStream\n    .schema(jsonSchema)               # Set the schema of the JSON data\n    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n    .json(inputPath)\n)\n\nstreamingCountsDF = (\n  streamingInputDF\n    .groupBy(\n      streamingInputDF.action,\n      window(streamingInputDF.time, \"1 hour\"))\n    .count()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67597e67-3502-4486-b8c0-1b0720cf4af3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 2.3. Start the Streaming Job\nYou start a streaming computation by defining a sink and starting it. In our case, to query the counts interactively, set the complete set of 1 hour counts to be in an in-memory table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6aced223-292d-4363-a26c-f4ae1408f928"}}},{"cell_type":"code","source":["query = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")        # memory = store in-memory table (for testing only)\n    .queryName(\"counts\")     # counts = name of the in-memory table\n    .outputMode(\"complete\")  # complete = all the counts should be in the table\n    .start()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a07466d-945a-4dbe-9d34-ec0b2c6f877a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 2.4. Interactively Query the Stream\nWe can periodically query the counts aggregation:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82aa36fe-a606-482e-ae0f-eb61b6ff1267"}}},{"cell_type":"code","source":["%sql\nSELECT action\n  , date_format(window.end, \"MMM-dd HH:mm\") AS time\n  , count\nFROM counts\nORDER BY time, action"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"158c0909-dd0a-4d0a-9276-452acb5f8330"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Close","Jul-26 03:00",11],["Open","Jul-26 03:00",179],["Close","Jul-26 04:00",344],["Open","Jul-26 04:00",1001],["Close","Jul-26 05:00",815],["Open","Jul-26 05:00",999],["Close","Jul-26 06:00",1003],["Open","Jul-26 06:00",1000],["Close","Jul-26 07:00",1011],["Open","Jul-26 07:00",993],["Close","Jul-26 08:00",989],["Open","Jul-26 08:00",1008],["Close","Jul-26 09:00",985],["Open","Jul-26 09:00",996],["Close","Jul-26 10:00",983],["Open","Jul-26 10:00",1000],["Close","Jul-26 11:00",1022],["Open","Jul-26 11:00",1007],["Close","Jul-26 12:00",1028],["Open","Jul-26 12:00",991],["Close","Jul-26 13:00",960],["Open","Jul-26 13:00",996],["Close","Jul-26 14:00",1028],["Open","Jul-26 14:00",1006],["Close","Jul-26 15:00",994],["Open","Jul-26 15:00",991],["Close","Jul-26 16:00",988],["Open","Jul-26 16:00",1020],["Close","Jul-26 17:00",984],["Open","Jul-26 17:00",992],["Close","Jul-26 18:00",1036],["Open","Jul-26 18:00",990],["Close","Jul-26 19:00",1001],["Open","Jul-26 19:00",1004],["Close","Jul-26 20:00",967],["Open","Jul-26 20:00",998],["Close","Jul-26 21:00",1035],["Open","Jul-26 21:00",1010],["Close","Jul-26 22:00",995],["Open","Jul-26 22:00",998],["Close","Jul-26 23:00",325],["Open","Jul-26 23:00",317]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"action","type":"\"string\"","metadata":"{}"},{"name":"time","type":"\"string\"","metadata":"{}"},{"name":"count","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>action</th><th>time</th><th>count</th></tr></thead><tbody><tr><td>Close</td><td>Jul-26 03:00</td><td>11</td></tr><tr><td>Open</td><td>Jul-26 03:00</td><td>179</td></tr><tr><td>Close</td><td>Jul-26 04:00</td><td>344</td></tr><tr><td>Open</td><td>Jul-26 04:00</td><td>1001</td></tr><tr><td>Close</td><td>Jul-26 05:00</td><td>815</td></tr><tr><td>Open</td><td>Jul-26 05:00</td><td>999</td></tr><tr><td>Close</td><td>Jul-26 06:00</td><td>1003</td></tr><tr><td>Open</td><td>Jul-26 06:00</td><td>1000</td></tr><tr><td>Close</td><td>Jul-26 07:00</td><td>1011</td></tr><tr><td>Open</td><td>Jul-26 07:00</td><td>993</td></tr><tr><td>Close</td><td>Jul-26 08:00</td><td>989</td></tr><tr><td>Open</td><td>Jul-26 08:00</td><td>1008</td></tr><tr><td>Close</td><td>Jul-26 09:00</td><td>985</td></tr><tr><td>Open</td><td>Jul-26 09:00</td><td>996</td></tr><tr><td>Close</td><td>Jul-26 10:00</td><td>983</td></tr><tr><td>Open</td><td>Jul-26 10:00</td><td>1000</td></tr><tr><td>Close</td><td>Jul-26 11:00</td><td>1022</td></tr><tr><td>Open</td><td>Jul-26 11:00</td><td>1007</td></tr><tr><td>Close</td><td>Jul-26 12:00</td><td>1028</td></tr><tr><td>Open</td><td>Jul-26 12:00</td><td>991</td></tr><tr><td>Close</td><td>Jul-26 13:00</td><td>960</td></tr><tr><td>Open</td><td>Jul-26 13:00</td><td>996</td></tr><tr><td>Close</td><td>Jul-26 14:00</td><td>1028</td></tr><tr><td>Open</td><td>Jul-26 14:00</td><td>1006</td></tr><tr><td>Close</td><td>Jul-26 15:00</td><td>994</td></tr><tr><td>Open</td><td>Jul-26 15:00</td><td>991</td></tr><tr><td>Close</td><td>Jul-26 16:00</td><td>988</td></tr><tr><td>Open</td><td>Jul-26 16:00</td><td>1020</td></tr><tr><td>Close</td><td>Jul-26 17:00</td><td>984</td></tr><tr><td>Open</td><td>Jul-26 17:00</td><td>992</td></tr><tr><td>Close</td><td>Jul-26 18:00</td><td>1036</td></tr><tr><td>Open</td><td>Jul-26 18:00</td><td>990</td></tr><tr><td>Close</td><td>Jul-26 19:00</td><td>1001</td></tr><tr><td>Open</td><td>Jul-26 19:00</td><td>1004</td></tr><tr><td>Close</td><td>Jul-26 20:00</td><td>967</td></tr><tr><td>Open</td><td>Jul-26 20:00</td><td>998</td></tr><tr><td>Close</td><td>Jul-26 21:00</td><td>1035</td></tr><tr><td>Open</td><td>Jul-26 21:00</td><td>1010</td></tr><tr><td>Close</td><td>Jul-26 22:00</td><td>995</td></tr><tr><td>Open</td><td>Jul-26 22:00</td><td>998</td></tr><tr><td>Close</td><td>Jul-26 23:00</td><td>325</td></tr><tr><td>Open</td><td>Jul-26 23:00</td><td>317</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b079bd57-7582-4a29-9c58-67b3847a948c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"00-Quick-Start","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2377260701751406}},"nbformat":4,"nbformat_minor":0}

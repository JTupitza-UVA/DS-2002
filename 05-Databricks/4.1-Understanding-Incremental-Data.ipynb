{"cells":[{"cell_type":"markdown","source":["### Understanding Incremental Data\n\nSpark Structured Streaming extends the functionality of Apache Spark to allow for simplified configuration and bookkeeping when processing incremental datasets. In the past, much of the emphasis for streaming with big data has focused on reducing latency to provide near real time analytic insights. While Structured Streaming provides exceptional performance in achieving these goals, this lesson will focus more on the applications of incremental data processing.\n\nWhile incremental processing is not absolutely necessary to work successfully in the data lakehouse, experience has shown that many workloads can benefit substantially from an incremental processing approach. To that end, many of Databricks' core features have been optimized specifically for handling these ever-growing datasets.\n\nConsider the following datasets and use cases:\n* Data scientists need secure, de-identified, versioned access to frequently updated records in an operational database\n* Credit card transactions need to be compared to past customer behavior to identify and flag fraud\n* A multi-national retailer seeks to serve custom product recommendations using purchase history\n* Log files from distributed systems need to be analayzed to detect and respond to instabilities\n* Clickstream data from millions of online shoppers needs to be leveraged for A/B testing of UX\n\nThese are just a small sample of datasets that grow incrementally and infinitely over time.  Here, we demonstrate the basics of using Spark Structured Streaming for incremental data processing.\n\n#### Objectives\n* Describe the programming model used by Spark Structured Streaming\n* Configure required options to perform a streaming read on a source\n* Describe the requirements for end-to-end fault tolerance\n* Configure required options to perform a streaming write to a sink\n\nFirst, run the following cell to import the data and make various utilities available for our experimentation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e390aa02-eaa7-4623-812c-ac9b57cb557d"}}},{"cell_type":"code","source":["%run ./Includes/4.1-setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6548eff-9aa6-43e9-967d-9efb9730e5ad"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 1.0. Treating Infinite Data as a Table\nThe main benefit of Spark Structured Streaming is that it enables users to interact with ever-growing data sources as if they were simply a static table of records.\n\n<img src=\"http://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png\" width=\"800\"/>\n\nIn the graphic above, a **data stream** describes any data source that grows over time. New data in a data stream might correspond to:\n* A new JSON log file landing in cloud storage\n* Updates to a database captured in a CDC feed\n* Events queued in a pub/sub messaging feed\n* A CSV file of sales closed the previous day\n\nHistorically, to update the results of a continuous stream of real-time data, either the entire source dataset had to be completely reprocessed, or custom logic had to be implemented to identify and process only those files or records that had been added since the previous update was executed.  Structured Streaming enables defining a query against the data source to automatically detect new records and propagate them through previously defined logic. **Spark Structured Streaming is optimized on Databricks to integrate closely with Delta Lake and Auto Loader.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d6f4689-ead2-4f95-b7e7-e59c1036395d"}}},{"cell_type":"markdown","source":["#### 2.0. Basic Concepts\n\n- The developer defines an **input table** by configuring a streaming read against a **source**. The syntax for doing this is similar to working with static data.\n- A **query** is defined against the input table. Both the DataFrames API and Spark SQL can be used to easily define transformations and actions against the input table.\n- This logical query on the input table generates the **results table**. The results table contains the incremental state information of the stream.\n- The **output** of a streaming pipeline will persist updates to the results table by writing to an external **sink**. Generally, a sink will be a durable system such as files or a pub/sub messaging bus.\n- New rows are appended to the input table for each **trigger interval**. These new rows are essentially analogous to micro-batch transactions and will be automatically propagated through the results table to the sink.\n\n<img src=\"http://spark.apache.org/docs/latest/img/structured-streaming-model.png\" width=\"800\"/>\n\n\nFor more information, see the section in the <a href=\"http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#basic-concepts\" target=\"_blank\">Structured Streaming Programming Guide</a>."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3de10ccd-918b-4b45-b167-3287cc27cd7d"}}},{"cell_type":"markdown","source":["#### 3.0. End-to-End Fault Tolerance\nStructured Streaming ensures end-to-end exactly-once fault-tolerance guarantees through _checkpointing_ (discussed below) and <a href=\"https://en.wikipedia.org/wiki/Write-ahead_logging\" target=\"_blank\">Write Ahead Logs</a>.  Structured Streaming sources, sinks, and the underlying execution engine work together to track the progress of stream processing. If a failure occurs, the streaming engine attempts to restart and/or reprocess the data. For best practices on recovering from a failed streaming query see <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/production.html#recover-from-query-failures\" target=\"_blank\">docs</a>. Note: This approach _only_ works if the streaming source is replayable; replayable sources include cloud-based object storage and pub/sub messaging services.\n\nAt a high level, the underlying streaming mechanism relies on a couple approaches:\n* First, Structured Streaming uses checkpointing and write-ahead logs to record the offset range of data being processed during each trigger interval.\n* Next, the streaming sinks are designed to be _idempotent_â€”that is, multiple writes of the same data (as identified by the offset) do _not_ result in duplicates being written to the sink.\n\nTaken together, replayable data sources and idempotent sinks allow Structured Streaming to ensure **end-to-end, exactly-once semantics** under any failure condition.\n\n\n#### 4.0. Reading a Stream\nThe **`spark.readStream()`** method returns a **`DataStreamReader`** used to configure and query the stream.  In the previous lesson, we saw code configured for incrementally reading with Auto Loader. Here, we'll show how easy it is to incrementally read a Delta Lake table.  The code uses the PySpark API to incrementally read a Delta Lake table named **`bronze`** and register a streaming temp view named **`streaming_tmp_vw`**.\n\n**NOTE**: A number of optional configurations (not shown here) can be set when configuring incremental reads, the most important of which allows you to <a href=\"https://docs.databricks.com/delta/delta-streaming.html#limit-input-rate\" target=\"_blank\">limit the input rate</a>."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93b9e826-d6e5-4e34-8298-9656b38f8774"}}},{"cell_type":"code","source":["(spark.readStream\n    .table(\"bronze\")\n    .createOrReplaceTempView(\"streaming_tmp_vw\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7746d98-b6fe-45a9-b6ee-e9c3ee7176b5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["When we execute a query on a streaming temporary view, we'll continue to update the results of the query as new data arrives in the source.  Think of a query executed against a streaming temp view as an **always-on incremental query**.\n\n**NOTE**: Generally speaking, unless a human is actively monitoring the output of a query during development or live dashboarding, we won't return streaming results to a notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbefd836-c954-4ca1-a2eb-deea8bfae07f"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM streaming_tmp_vw"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2dbc6f5b-3433-4f35-bde4-85c09d59ac44"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You will recognize the data as being the same as the Delta table written out in our previous lesson.  Before continuing, click **`Stop Execution`** at the top of the notebook, **`Cancel`** immediately under the cell, or run the following cell to stop all active streaming queries."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"afe07285-7eac-4269-913a-bf3b163727cf"}}},{"cell_type":"code","source":["for s in spark.streams.active:\n    print(\"Stopping \" + s.id)\n    s.stop()\n    s.awaitTermination()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"471a3766-86f3-4049-ac73-15a20a819a3a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 5.0. Working with Streaming Data\nWe can execute most transformation against streaming temp views the same way we would with static data. Here, we'll run a simple aggregation to get counts of records for each **`device_id`**.  Because we are querying a streaming temp view, this becomes a streaming query that executes indefinitely, rather than completing after retrieving a single set of results. For streaming queries like this, Databricks Notebooks include interactive dashboards that allow users to monitor streaming performance. Explore this below. One important note regarding this example: this is merely displaying an aggregation of input as seen by the stream. **None of these records are being persisted anywhere at this point.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"896fc3cd-5da2-429c-8ae0-2a32a6cfa00f"}}},{"cell_type":"code","source":["%sql\nSELECT device_id, count(device_id) AS total_recordings\nFROM streaming_tmp_vw\nGROUP BY device_id"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"562760d9-6502-48bf-93cb-20ae7cd86ef2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Before continuing, click **`Stop Execution`** at the top of the notebook, **`Cancel`** immediately under the cell, or run the following cell to stop all active streaming queries."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3b5e023-a43e-48b9-87f2-5f5517b7358e"}}},{"cell_type":"code","source":["for s in spark.streams.active:\n    print(\"Stopping \" + s.id)\n    s.stop()\n    s.awaitTermination()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea592901-8a3d-490f-a7db-c79a2b0cbade"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 6.0. Unsupported Operations\n\nMost operations on a streaming DataFrame are identical to a static DataFrame. There are <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations\" target=\"_blank\">some exceptions to this</a>.  Consider the model of the data as a constantly appending table. Sorting is one of a handful of operations that is either too complex or logically not possible to do when working with streaming data.  A full discussion of these exceptions is out of scope for this course. Note that advanced streaming methods like windowing and watermarking can be used to add additional functionality to incremental workloads.\n\nUncomment and run the following cell how this failure may appear:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc54294d-b42e-499e-a744-a5b9f61a5263"}}},{"cell_type":"code","source":["# %sql\n# SELECT * \n# FROM streaming_tmp_vw\n# ORDER BY time"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9fc008d-0531-4496-963c-facb36682b23"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 7.0. Persisting Streaming Results\nIn order to persist incremental results, we need to pass our logic back to the PySpark Structured Streaming DataFrames API.  Above, we created a temp view from a PySpark streaming DataFrame. If we create another temp view from the results of a query against a streaming temp view, we'll again have a streaming temp view."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbd0a598-8f0e-4254-a041-17f0376ba9dc"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMP VIEW device_counts_tmp_vw AS (\n  SELECT device_id, COUNT(device_id) AS total_recordings\n  FROM streaming_tmp_vw\n  GROUP BY device_id\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5fd698b-eb49-412e-83e7-dbaa19e0f7ba"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 8.0. Writing a Stream\nTo persist the results of a streaming query, we must write them out to durable storage. The **`DataFrame.writeStream`** method returns a **`DataStreamWriter`** used to configure the output. When writing to Delta Lake tables, we typically will only need to worry about 3 settings, discussed here.\n\n\n##### 8.1. Checkpointing\nDatabricks creates checkpoints by storing the current state of your streaming job to cloud storage. Checkpointing combines with write ahead logs to allow a terminated stream to be restarted and continue from where it left off. Checkpoints cannot be shared between separate streams. A checkpoint is required for every streaming write to ensure processing guarantees.\n\n\n##### 8.2. Output Modes\nStreaming jobs have output modes similar to static/batch workloads. <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes\" target=\"_blank\">More details here</a>.\n\n| Mode   | Example | Notes |\n| ------------- | ----------- | --- |\n| **Append** | **`.outputMode(\"append\")`**     | **This is the default.** Only newly appended rows are incrementally appended to the target table with each batch |\n| **Complete** | **`.outputMode(\"complete\")`** | The Results Table is recalculated each time a write is triggered; the target table is overwritten with each batch |\n\n\n##### 8.3. Trigger Intervals\nWhen defining a streaming write, the **`trigger`** method specifies when the system should process the next set of data..\n\n| Trigger Type                           | Example | Notes |\n|----------------------------------------|-----------|-------------|\n| Unspecified                            |  | **This is the default.** This is equivalent to using **`processingTime=\"500ms\"`** |\n| Fixed interval micro-batches           | **`.trigger(processingTime=\"2 minutes\")`** | The query will be executed in micro-batches and kicked off at the user-specified intervals |\n| One-time micro-batch                   | **`.trigger(once=True)`** | The query will execute a single micro-batch to process all the available data and then stop on its own |\n\n*Note that triggers are specified when defining how data will be written to a sink and control the frequency of micro-batches. By default, Spark will automatically detect and process all data in the source that has been added since the last trigger.*\n\n\n#### 9.0 Pulling It All Together\nThe code below demonstrates using **`spark.table()`** to load data from a streaming temp view back to a DataFrame. Note that Spark will always load streaming views as a streaming DataFrame and static views as static DataFrames (meaning that incremental processing must be defined with read logic to support incremental writing). \n\nIn this first query, we'll demonstrate using **`trigger(once=True)`** to perform incremental batch processing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e3e2ab7-0068-45f5-a5d0-406bd4570f4a"}}},{"cell_type":"code","source":["(spark.table(\"device_counts_tmp_vw\")                               \n    .writeStream                                                \n    .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/silver\")\n    .outputMode(\"complete\")\n    .trigger(once=True)\n    .table(\"device_counts\")\n    .awaitTermination() # This optional method blocks execution of the next cell until the incremental batch write has succeeded\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"272af16b-939b-49d5-baa4-b973f103a107"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Below, we change our trigger method to change this query from a triggered incremental batch to an always-on query triggered every 4 seconds.\n\n**NOTE**: As we start this query, no new records exist in our source table. We'll add new data shortly."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89e20b77-99a7-4597-8528-d93abb8ea985"}}},{"cell_type":"code","source":["query = (spark.table(\"device_counts_tmp_vw\")                               \n              .writeStream                                                \n              .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/silver\")\n              .outputMode(\"complete\")\n              .trigger(processingTime='4 seconds')\n              .table(\"device_counts\"))\n\n# Like before, wait until our stream has processed some data\nDA.block_until_stream_is_ready(query)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c1e1a791-4607-4ec6-8b48-78e0b5db6416"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 10.0. Querying the Output\nNow let's query the output we've written from SQL. Because the result is a table, we only need to deserialize the data to return the results.  Because we are now querying a table (not a streaming DataFrame), the following will **not** be a streaming query."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4245a319-9d8a-418f-953a-5d031abc9392"}}},{"cell_type":"code","source":["%sql\nSELECT *\nFROM device_counts"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ecf9cd77-f087-4355-bf54-34831184c6f3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 11.0. Land New Data\n\nAs in our previous lesson, we have configured a helper function to process new records into our source table. Run the cell below to land another batch of data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5899b9ad-e1a4-4796-a18d-8d07009ba5d5"}}},{"cell_type":"code","source":["DA.data_factory.load()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8206458-893d-4a5b-87fb-9a2b95c34a1b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Query the target table again to see the updated counts for each **`device_id`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"753d6abc-dc6c-4886-89c1-32be90cfdb90"}}},{"cell_type":"code","source":["%sql\nSELECT *\nFROM device_counts"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"afd879ae-4cab-43e1-92ec-276368e5e692"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 12.0 Clean Up\nFeel free to continue landing new data and exploring the table results with the cells above.  When you're finished, run the following cell to stop all active streams and remove created resources before continuing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8164ca76-9863-4350-8a01-67f61fbc1fe3"}}},{"cell_type":"code","source":["DA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37a3a58c-5b71-4eb5-9064-f2b2b9a8041d"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"4.1-Understanding-Incremental-Data","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2022558117408450}},"nbformat":4,"nbformat_minor":0}
